
%% bare_adv.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the advanced use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin




%
\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)



% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx,float,url,multirow,makecell,booktabs,color,subfigure,caption,setspace}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}


\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newtheorem{theorem}{$\textbf{Theorem}$}
\newtheorem{lemma}[theorem]{$\textbf{Lemma}$}
\newtheorem{proposition}[theorem]{$\textbf{Proposition}$}

% Strut macros for skipping spaces above and below text in tables.
\def\abovestrut#1{\rule[0in]{0in}{#1}\ignorespaces}
\def\belowstrut#1{\rule[-#1]{0in}{#1}\ignorespaces}
\def\abovespace{\abovestrut{0.01in}}
\def\belowspace{\belowstrut{-0.01in}}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/acronym


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/pkg/mdwtools


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/pkg/eqparbox




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% NOTE: PDF thumbnail features are not required in IEEE papers
%       and their use requires extra complexity and work.
%\ifCLASSINFOpdf
%  \usepackage[pdftex]{thumbpdf}
%\else
%  \usepackage[dvips]{thumbpdf}
%\fi
% thumbpdf.sty and its companion Perl utility were written by Heiko Oberdiek.
% It allows the user a way to produce PDF documents that contain fancy
% thumbnail images of each of the pages (which tools like acrobat reader can
% utilize). This is possible even when using dvi->ps->pdf workflow if the
% correct thumbpdf driver options are used. thumbpdf.sty incorporates the
% file containing the PDF thumbnail information (filename.tpm is used with
% dvips, filename.tpt is used with pdftex, where filename is the base name of
% your tex document) into the final ps or pdf output document. An external
% utility, the thumbpdf *Perl script* is needed to make these .tpm or .tpt
% thumbnail files from a .ps or .pdf version of the document (which obviously
% does not yet contain pdf thumbnails). Thus, one does a:
%
% thumbpdf filename.pdf
%
% to make a filename.tpt, and:
%
% thumbpdf --mode dvips filename.ps
%
% to make a filename.tpm which will then be loaded into the document by
% thumbpdf.sty the NEXT time the document is compiled (by pdflatex or
% latex->dvips->ps2pdf). Users must be careful to regenerate the .tpt and/or
% .tpm files if the main document changes and then to recompile the
% document to incorporate the revised thumbnails to ensure that thumbnails
% match the actual pages. It is easy to forget to do this!
%
% Unix systems come with a Perl interpreter. However, MS Windows users
% will usually have to install a Perl interpreter so that the thumbpdf
% script can be run. The Ghostscript PS/PDF interpreter is also required.
% See the thumbpdf docs for details. The latest version and documentation
% can be obtained at.
% http://www.ctan.org/pkg/thumbpdf


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!
%\ifCLASSINFOpdf
%\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
%\else
%\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
%\usepackage{breakurl}
%\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.).
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
% That said, Vilar Camara Neto created his breakurl.sty package which
% permits hyperref to easily break URLs even in dvi mode.
% Note that breakurl, unlike most other packages, must be loaded
% AFTER hyperref. The latest version of breakurl and its documentation can
% be obtained at:
% http://www.ctan.org/pkg/breakurl
% breakurl.sty is not for use under pdflatex pdf mode.
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/pkg/hyperref





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Modelling Attention in Panoramic Video: \\A Reinforcement Learning Approach}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
        John~Doe,~\IEEEmembership{Fellow,~OSA,}
        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
GA, 30332.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: see http://www.michaelshell.org/contact.html
\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Panoramic video provides immersive and interactive experience, by enabling human to control field of view (FoV) with head movement (HM).
Thus, HM plays a key role in modelling human attention on panoramic video.
In this paper, we establish a database collecting subjects' HM positions on panoramic video sequences, and we find from our database that the HM data are highly consistent across subjects. We further find that deep reinforcement learning (DRL) can be applied in predicting HM positions, seen as actions of an agent.
Based on our findings, we propose a DRL based HM prediction (DHP) approach in offline and online versions, called offline-DHP and online-DHP, respectively.
In offline-DHP, multiple DRL workflows are run to decide some potential HM positions on each panoramic video frame.
Then, the heat map of the predicted HM positons, called the HM map, is generated as the output of offline-DHP.
In online-DHP, the next HM position of one subject is estimated, given the observed HM positions of this subject at the previous frames.
Such online estimation is achieved by developing a DRL algorithm, which is based on the learned model of offline-DHP.
Finally, experimental results validate that offline-DHP and online-DHP are effective in predicting HM positions of panoramic video in offline and online manners, respectively. Experimental results also show that the learned offline-DHP model is capable of improving
the performance of online-DHP.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Computer Society, IEEE, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{Introduction}}
\else
\section{Introduction}
\label{Introduction}
\fi
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{D}{uring} the past years, panoramic video \cite{neumann2000immersive} has been increasingly popular, due to its immersive and interactive experience.
To achieve the immersive and interactive experience, human can control field of view (FoV) via wearing head mounted displays (HMD), when watching panoramic video in a range of $360^{\circ} \times 180^{\circ}$.
In other words, humans are able to freely move their heads within a sphere, to make their FoVs focus on the attractive content (see Figure \ref{fig-one} for an example).
The content outside FoV cannot be seen by humans, i.e., without any attention.
Consequently, head movement (HM) plays a key role in deploying human attention on panoramic video.
HM prediction thus emerges as an increasingly important problem in modelling attention in panoramic video.
Given the predicted HM, visual attention within FoV can be further modelled by the state-of-the-art saliency detection methods \cite{borji2013state}.
The same as traditional 2D video, attention model can be extensively utilized in many areas of panoramic video, such as region-of-interest (ROI) compression \cite{de2016video}, visual quality assessment \cite{gaddam2016tiling}, rendering \cite{stengel2016gaze}, synopsis \cite{Pritch08}, and automatic cinematography \cite{su2016pano2vid}.

Unfortunately, few work has been proposed to model human attention on panoramic video, especially predicting the positions of HM.
Benefitting from the most recent success of deep reinforcement learning (DRL) \cite{mnih2016asynchronous}, this paper proposes a DRL based HM prediction (DHP) approach for modelling attention on panoramic video.
HM prediction can be classified into two categories: offline and online manners.
The offline HM prediction refers to modelling attention on panoramic video for multiple subjects, while the online prediction means predicting the next HM position of one subject upon the ground-truth of his/her HM positions at the current and previous frames. In this paper, our DHP approach includes both online and offline HM prediction, named as offline-DHP and online-DHP, respectively.

\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=1.6\columnwidth]{figures/introduction/fig_one}}
		\caption{Illustration for FoVs and HM positions across different subjects. The heat map of HM positions from all subjects is also shown, which is defined as the HM map.}
		\label{fig-one}
	\end{center}
\end{figure*}

To our best knowledge, there exists no offline work to predict HM positions of multiple subjects in viewing panoramic video. The closest work is saliency detection on 2D video \cite{borji2013state}. The earliest approach for saliency detection was proposed by \textit{ltti et al.} \cite{itti1998model}, in which the features of color, intensity and orientation are combined to generate the saliency map of an image. Later, \textit{ltti et al.} \cite{itti2004automatic} proposed to add two features in \cite{itti1998model}, motion and flicker contrast, for video saliency detection. Recently, several advanced approaches have been proposed for video saliency prediction. These advanced works include  the earth mover's distance (EMD) approach \cite{lin2013visual} and the boolean map based saliency model (BMS) \cite{zhang2016exploiting}.
Most recently, deep learning has been successfully applied in video saliency detection,
such as SALICON \cite{huang2015salicon} and Liu's approach \cite{Liu2017cvpr}.
Saliency detection in 2D video assumes that humans are able to view all content of each video frame.
However, this assumption does not hold for panoramic video, as subjects can only see a limited range of FoV at a single sight, rather than the full panoramic range of $360^{\circ} \times 180^{\circ}$.
In fact, different regions of panoramic video are accessible to subjects via changing the positions of HM \cite{lowe2015visualization}.
In this paper, we find that different subjects are highly consistent on HM positions.
Such finding is based on establishing and analyzing a new database, which consists of 58 subjects' HM data in viewing 76 panoramic video sequences.
Then, we propose the offline-DHP approach to predict the consistent HM positions on panoramic video via generating the HM map for each single frame.
The HM maps are in the form of sphere, and the positions in the HM maps are represented by the longitude and latitude in the geographic coordinate system (GDS) \cite{Goodchild2007}. This paper visualizes the spherical HM maps by projecting them to the 2D plane.
Figure \ref{fig-one} demonstrates an example of the ground-truth HM map for a panoramic video frame. Similar to the saliency maps of 2D video, the HM maps of panoramic video are obtained by convoluting HM positions with the 2D Gaussian filter\footnote{The two dimensions of the Gaussian filter are longitude and latitude.}.

%Then, we  propose a DRL based HM prediction (DHP) approach, which predicts the possible HM positions on panoramic video via generating the HM map for each single frame.
%Here, the HM maps of panoramic videos are similar to the saliency maps of 2D video.
%Figure \ref{fig-one} demonstrates an example of the ground-truth HM map for a panoramic video frame.
%As shown in this figure, HM positions of multiple subjects need to be predicted in an offline manner, such that the HM maps can be obtained.
%Therefore, our DHP approach is offline for yielding HM maps, namely off-line DHP.

Specifically, our offline-DHP approach yields the HM maps of panoramic video via predicting HM scan-paths of multiple \textit{agents}, since subjects interactively control their HM positions along with some scan-paths according to video content.
First, we find from our database that the HM scan-paths of different subjects are with high consistency.
Meanwhile, subjects are normally initialized to view the center of front region in the beginning frames of panoramic video.
Thereby, HM positions at subsequent frames can be yielded on the basis of the predicted scan-paths.
Additionally, we find from our database that the magnitudes and directions of HM scan-paths are with similarity across subjects.
In light of these findings, our offline-DHP approach models both magnitudes and directions of HM scan-paths as \textit{actions} of multiple \textit{agents} and takes viewed panoramic content as the \textit{observation} of \textit{environment}.
As such, the DRL model of HM prediction  can be learned for HM prediction.
In training the DRL model, \textit{reward} is designed to measure the difference of \textit{actions} of HM scan-paths between the DRL \textit{agents} and subjects. Then, the \textit{reward} is optimized to learn parameters in the DRL model.
Given the learned model, HM maps of panoramic video are generated upon HM positions, obtained from scan-paths of several \textit{agents} in multiple DRL workflows.

%we propose a novel offline-DHP approach, as our offline approach for predicating HM positions on panoramic video.

For online HM prediction, the latest work of \cite{hu2017deep} proposed a deep 360 pilot, which automatically shifts viewing direction (equivalent to HM position) in watching panoramic video. Specifically, the salient object is detected and tracked across panoramic video frames, via leveraging region-based convolutional neural network (RCNN) \cite{ren2015faster} and recurrent neural network (RNN). Given the detected salient object and previous viewing directions, the deep 360 pilot predicts to transit HM position by learning a regressor. Since the deep 360 pilot relies heavily on one salient object, it is only suitable for some specific scenes that include one salient object, e.g., the sport scenes in \cite{hu2017deep}. It is still challenging to predict HM positions online for generic panoramic video, which may include more than one salient object (e.g., the panoramic video of Figure \ref{fig-one}). In this paper, we propose an online approach, namely online-DHP, to predict the HM positions on generic panoramic video, in terms of longitude and latitude. Different from \cite{hu2017deep}, our online-DHP approach does not need to detect the salient object of RCNN. Instead, it is based on attention-related content by leveraging the learned model of our offline-DHP approach. Then, a DRL algorithm is developed to predict the HM positions in an online manner. Specifically, in the DRL algorithm, the \textit{agent} predicts the \textit{action} of HM scan-path in the next frame, according to ground-truth of the previous HM scan-path and \textit{observation} of video content. Consequently, the HM positions at the incoming frames can be predicted for our online-DHP approach.


The main contributions of this paper are three-fold:
\begin{itemize}
\item We establish a new panoramic video database comprising HM positions of 58 subjects, with a thorough analysis on their HM data across 76 panoramic video sequences.

\item We propose an offline-DHP approach to detect HM maps of panoramic video, which predicts the consistent HM positions of multiple subjects.

\item We develop an online-DHP approach to predict the HM position of one subject at the next panoramic frame, upon video content and HM positions at the current and previous frames.

\end{itemize}

%based on content of the current frame and knowledge of viewing directions in the previous frames.

%To our best knowledge, our work is a pioneering one in modeling attention on panoramic video using HM maps.




%To the best of our knowledge, there exists few work to model attention of subjects, especially their HMs, on panoramic video.
%The closest work is saliency detection on 2D video \cite{borji2013state}.
%Video saliency detection can be traced back to \cite{itti2004automatic}, in which \textit{ltti et al.} proposed to generate saliency maps of video frames, by linearly combining the conspicuity maps of feature channels on color, intensity, orientation, motion and flicker contrast.
%Later, some advanced approaches on saliency detection were proposed, such as \cite{itti2006bayesian, Hou2008dynamic, guo2010novel, lin2013visual, rudoy2013learning, hossein2015many, zhang2016exploiting, xu2017learning}.
%For example, Lin \textit{et al.} \cite{lin2013visual} applied earth mover's distance (EMD) to measure the center-surround difference in spatio-temporal receptive field, for yielding the dynamic saliency maps of videos.
%Hossein \textit{et al.} \cite{hossein2015many} proposed to predict video saliency upon bit allocation of each block in video coding.
%Most recently, deep learning \cite{bazzani2016recurrent, bak2016two, Liu2017cvpr} has been applied in video saliency detection.
%For example, Bazzani \textit{et al.} \cite{bazzani2016recurrent} proposed a recurrent mixture density model, which combines 3D convolutional neural network (CNN) and long short-term memory network (LSTM), for predicting video saliency.
%
%Saliency detection in 2D video assumes that humans are able to view all content of each video frame.
%This assumption does not hold for panoramic videos, as subjects can only see a limited range of FoV at a single sight, rather than the full panoramic range of $360^{\circ} \times 180^{\circ}$.
%In fact, different regions of panoramic video are accessible to subjects via changing the positions of HM \cite{lowe2015visualization}.
%For panoramic video of sport scenes, the most recent works \cite{lin2017tell, hu2017deep} proposed to predict the current HM position of one subject (called viewing angle in \cite{hu2017deep}) upon observed HM positions of this subject at previous frames.
%These works belong to online attention modeling of a single subject.
%Unfortunately, there is no offline approach to model attention of multiple subjects on panoramic video.
%In this paper, we establish a database consisting of 40 subjects' HM data on viewing 48 panoramic video sequences. Based on the established database, we argue that different subjects are consistent on HM positions, i.e., the longitude and latitude of their viewing directions in a panorama are similar.
%Hence, we propose an offline approach to predict HM positions of panoramic video via generating the HM map for each single frame, which is similar to saliency maps of 2D video.
%Figure 1 demonstrates an example of the ground-truth HM map for a panoramic video frame.
%To our best knowledge, our work is a pioneering one in modeling attention on panoramic video using HM maps.




\section{Related work}

\subsection{Saliency detection}
The only approach on predicting HM positions of panoramic videos is the most recent work of \cite{su2016pano2vid}, in which Pano2Vid was proposed to yield FoV at each panoramic video frame. However, Pano2Vid mainly focuses on virtually generating a potential HM position at one frame, rather than modelling HM maps of multiple subjects at this frame. The closest work on predicting HM maps is saliency detection for 2D video, which is briefly reviewed in the following.

Saliency detection aims to predict visual attention of humans on 2D videos, by generating saliency maps of video frames. The studies on visual saliency start from images in 1998, when Itti and Koch \cite{itti1998model} found that the features of intensity, color and orientation in an image can be employed to detect its saliency map. Afterwards, they extended their work to video saliency detection \cite{itti2004automatic}, in which two dynamic features of motion and flicker contrast are combined with \cite{itti1998model} for detecting saliency in 2D videos. Both \cite{itti1998model} and \cite{itti2004automatic} can be seen as heuristic approaches for saliency detection, since they make use of the understanding of the HVS to develop the computational models. Recently, some advanced heuristic approaches, e.g.,  \cite{itti2009bayesian, boccignone2008nonparametric, zhang2009sunday, guo2010novel, ren2013regularized, lin2013visual, zhang2016exploiting, hossein2015many, xu2017learning}, have been proposed for saliency detection in 2D videos. Specifically, \cite{itti2009bayesian} proposed a novel feature called \textit{surprise}, which measures how the visual change attracts human observers, based on the Kullback-Leibler (KL) divergence between spatio-temporal posterior and prior beliefs. Given the feature of \textit{surprise}, a Bayesian framework was developed in \cite{itti2009bayesian} for video saliency detection.  Some other Bayesian frameworks \cite{boccignone2008nonparametric, zhang2009sunday}  were also developed for detecting video saliency. Besides, Lin \textit{et al.} \cite{lin2013visual} quantified earth mover's distance (EMD) to measure the center-surround difference in spatio-temporal receptive filed, generating saliency maps for 2D videos. Zhang \textit{et al.} \cite{zhang2016exploiting} explored the surround cue for saliency detection, by characterizing a set of binary images with randomly thresholds on color channels. Recently, \cite{hossein2015many} and \cite{xu2017learning} have investigated that some features (e.g., motion vector) in compressed domain are of high correlation with human attention, thus being explored in video saliency detection.

Benefitting from the most recent success of deep learning, deep neural networks (DNNs) \cite{huang2015salicon, kruthiventi2015deepfix, wang2016RCNN, bazzani2016recurrent, Liu2017cvpr, bak2016two,wang2017deep} have also been  developed to detect 2D video saliency, instead of exploring the HVS related features in heuristic saliency detection approaches. They can be seen as data-driven approaches. For static saliency detection, SALICON [19] fine tuned the existing convolutional neural networks (CNN), with a new saliency related loss function. For dynamic saliency detection, \cite{bazzani2016recurrent} leveraged a deep Convolutional 3D (C3D) network to learn the representations of human attention on 16 consecutive frames, and then a Long Short-Term Memory (LSTM) network connected with a mixture density network was learned to generate saliency maps in Gaussian mixture distribution. Similarly, Liu \textit{et al.} \cite{Liu2017cvpr} combined CNN and multi-stream LSTM for detecting saliency in videos with multiple faces. Besides, other DNN structures have been developed to detect either static saliency \cite{kruthiventi2015deepfix, wang2016RCNN} or dynamic saliency \cite{bak2016two,bazzani2016recurrent,wang2017deep}.


Although saliency detection has been thoroughly studied for predicting eye movement on 2D videos, there is no work on the prediction of HM positions on panoramic videos.
Similar to saliency detection in 2D videos, this paper proposes to generate HM maps, which represent HM positions of multiple subjects, for modelling attention on panoramic videos. Towards the HM maps of panoramic videos, a DRL approach is developed to estimate the \textit{actions} of HM by multiple \textit{agents} upon the \textit{environment} of panoramic video content, the features of which are automatically learned and then extracted by DNN. Thus, our approach takes advantage of both deep learning and reinforcement learning, driven by the HM data of our panoramic video database.
It is worth mentioning that although few work applies DRL to predict human attention, attention model is widely used in the opposite direction, to improve the performance of reinforcement learning, e.g., \cite{minut2001reinforcement, mnih2014recurrent, jaderberg2016reinforcement, wang2016dueling}.


\subsection{Virtual cinematography}
Virtual cinematography of panoramic videos was proposed in \cite{foote2000flycam, sun2005region, su2016pano2vid, hu2017deep, lin2017tell}, which directs an imaginary camera to virtually capture natural FOV (NFOV). In general, virtual cinematography attempts to agree with HM positions of humans at each panoramic video frame. The early work of \cite{foote2000flycam} proposed cropping object-of-interest in panoramic videos, such that NFOV can be generated for virtual cinematography. Later, in \cite{sun2005region} the cropped object-of-interest is tracked across frames by a Kalman filter, for automatically controlling virtual camera in virtual cinematography of panoramic videos. The approach of \cite{sun2005region} can work on both compressed and uncompressed domains, as two methods were developed for detecting object-of-interest in compressed and uncompressed domains, respectively. Both the works of \cite{foote2000flycam, sun2005region} were designed for the task of online virtual cinematography. They can be seen as heuristic approaches, which are not trained or even evaluated on the ground-truth HM data of humans.

Most recently, data-driven approaches boost the development of virtual cinematography for panoramic videos. Specifically, Pano2Vid \cite{su2016pano2vid} learns to generate NFOV at each panoramic frame. However, the learning mechanism of Pano2Vid is offline. In fact, NFOV can be estimated at each frame in an online manner, which uses observed HM positions of the previous frames to correct the estimation of NFOV at the current frame. To this end, online virtual cinematography has been studied \cite{hu2017deep, lin2017tell} in a data-driven way, in which HM positions are predicted by an online mechanism.
Specifically, the deep 360 pilot was proposed in \cite{hu2017deep}, which is a deep learning based \textit{agent} smoothly tracking object-of-interest for panoramic video. In other words, the \textit{agent} transits the HM position across video frames to track the key object detected by RCNN, given the observed HM position at previous frames. Consequently, the NFOV can be generated online for automatically displaying object-of-interest in virtual cinematography of panoramic videos. In fact, object-of-interest tracking in panoramic videos refers to continuously focusing and refocusing intended targets, respectively. Both focusing and re-focusing require a subject to catch up the object. Such a task is challenging in extreme-sport videos, as the object-of-interest may be moving fast. Therefore, Lin \textit{et. al} \cite{lin2017tell} investigated two focus assistance techniques to help the subject track the key object in viewing panoramic videos, in which the potential HM position attended to the object-of-interest needs to determined and provided for the subject.

The above approaches of \cite{foote2000flycam, sun2005region, su2016pano2vid, hu2017deep, lin2017tell}  all depend on the detector of object-of-interest. Thus, they can be only applied for some specific panoramic videos with salient objects, such as video conferencing or classroom scenes in \cite{foote2000flycam, sun2005region} and the sports videos in \cite{su2016pano2vid, hu2017deep, lin2017tell}. Different from these conventional approaches, our online HM prediction approach is based on the learned model of our offline approach, which encodes HM related content rather than detecting object-of-interest. Consequently, our approach is object-free, thus more suitable for the generic panoramic videos.

%Both the works of \cite{lin2017tell, hu2017deep} are designed for the task of virtual cinematography, which directs an imaginary camera to virtually capture NFOV from panoramic videos. Also, the work of \cite{su2016pano2vid} also targets at virtual cinematography of panoramic video, but in an offline manner. In fact, most of existing virtual cinematography works \cite{} concentrate on camera manipulation within a simple virtual environment, rather than panoramic scenes.





%The initial work of online HM prediction can be traced back to automatic cinematography of panoramic video \cite{}.


\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/database/consistence-magnitude}}
		\caption{\footnotesize{Mean and standard deviation for the HM scan-path magnitude over different subjects, for 76 panoramic video sequences in the PVS-HM database. The last column also shows the mean and standard deviation, averaged over all sequences. }}
		\label{consistence-magnitude}
	\end{center}
\end{figure*}


\begin{figure*}
\vspace{-2em}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/database/direction-consistence}}%of
		\caption{\footnotesize{Circular standard deviation for the direction of HM scan-paths, over 76 panoramic video sequences in the PVS-HM database. The average results are also shown in the last column.}}
		\label{direction-consistence}
	\end{center}
\end{figure*}

\section{Database establishment and analysis}
\label{Database_establishment_and_analysis}

\subsection{Database establishment}
\label{Database_establishment}

In this section, we collect a new database including 76 panoramic video sequences with HM data of 58 subjects, called PVS-HM database. Our PVS-HM database allows quantitative analysis of human's HM on panoramic video, and it can be also used for learning to predict where human looks at panoramic video. Our database is available in  (XXX website XXX) for facilitating the future research. In the following, we present how we conducted the experiment to obtain the PVS-HM database.

First, we selected 76 panoramic video sequences from YouTube and VRCun, with resolution ranging from 3K to 8K.
As seen in Table \ref{tab:CC}, the content of these sequences are diverse, including computer animation (CA), driving, action sports, movie, video game, scenery, etc.
Then, the duration of each sequence was cut to be 10 to 80 seconds (averagely 26.9 seconds), such that fatigue can be reduced in viewing panoramic video.
To ensure video quality, all panoramic video sequences were compressed by H.265 \cite{Sullivan2013Overview} without any change at bit-rates.
Note that the audio tracks were removed to avoid the impact of acoustic information on visual attention.

In our experiment, 58 subjects (41 males and 17 females, aging from 18 to 36) weared the HMD of HTC Vive to view all 76 panoramic video sequences at random display order.
When viewing panoramic video, the subjects seated on a swivel chair were allowed to turn around freely, such that all panoramic regions are accessible.
To avoid eye fatigue and motion sickness, the subjects have a 5 minute rest after viewing each session of 19 sequences.
With the support of the software development kit (SDK) of HTC Vive, we recorded the posture data of each subject when viewing panoramic video.
Based on the recorded posture data, HM data of all 58 subjects at each frame of the panoramic video sequences were obtained and stored for our PVS-HM database, in terms of longitude and latitude in the GDS.


\subsection{Database analysis}
\label{Database_analysis}

In this section, we mine our PVS-HM database to analyze HM data of different subjects across panoramic video sequences.
Specifically, we have the following five findings.

%\begin{table*}
%\begin{center}
%  \caption{CC between ground-truth HM maps of Groups $A$ and $B$, for each panoramic video sequence} \label{tab:CC}
%  \tiny
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{*{4}{|c|c|c}|}
%  \hline
%  \tabincell{c}{Cate-\\gory} & Name & CC & \tabincell{c}{Cate-\\gory} & Name & CC & \tabincell{c}{Cate-\\gory} & Name & CC & \tabincell{c}{Cate-\\gory} & Name & CC \\
%  \hline
%  \multirow{6}{*}{\rotatebox{90}{CA}} & AcerPredator & 0.839$\pm$0.087
%&
%  \multirow{6}{*}{\rotatebox{90}{Driving}} & AirShow & 0.783$\pm$0.078
% &
%  \multirow{6}{*}{\rotatebox{90}{Others}} & A380 & 0.839$\pm$0.106
% &
%  \multirow{6}{*}{\rotatebox{90}{Video Game}} & CS & 0.819$\pm$0.084
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & BFG & 0.644$\pm$0.146
% & & DrivingInAlps & 0.857$\pm$0.071
% & & CandyCarnival & 0.723$\pm$0.094
% & & Dota2 & 0.714$\pm$0.103
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & CMLauncher & 0.828$\pm$0.119
% & & F5Fighter & 0.592$\pm$0.126
% & & MercedesBenz & 0.592$\pm$0.133
% & & GalaxyOnFire & 0.762$\pm$0.084
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & Cryogenian & 0.526$\pm$0.174
% & & HondaF1 & 0.872$\pm$0.053
% & & RingMan & 0.897$\pm$0.054
% & & LOL & 0.724$\pm$0.097
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & LoopUniverse & 0.779$\pm$0.078
% & & Rally & 0.867$\pm$0.047
% & & RioOlympics & 0.624$\pm$0.123
% & & MC & 0.726$\pm$0.115
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & Pokemon & 0.607$\pm$0.182
% & & Supercar & 0.854$\pm$0.064
% & & VRBasketball & 0.770$\pm$0.105
% & & SuperMario64 & 0.860$\pm$0.054
% \\
%  \hline
%  \multirow{6}{*}{\rotatebox{90}{Movie}} & Help & 0.859$\pm$0.122
% &
%  \multirow{6}{*}{\rotatebox{90}{Scenery}} & Antarctic & 0.674$\pm$0.135
% &
%  \multirow{6}{*}{\rotatebox{90}{Show}} & BTSRun & 0.867$\pm$0.061
% &
%  \multirow{6}{*}{\rotatebox{90}{Action Sports}} & Gliding & 0.528$\pm$0.158
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & IRobot & 0.771$\pm$0.078
% & & BlueWorld & 0.559$\pm$0.156
% & & Graffiti & 0.807$\pm$0.100
% & & Parachuting & 0.628$\pm$0.157
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & Predator & 0.696$\pm$0.124
% & & Dubai & 0.646$\pm$0.133
% & & KasabianLive & 0.722$\pm$0.132
% & & RollerCoaster & 0.834$\pm$0.078
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & ProjectSoul & 0.918$\pm$0.053
% & & Egypt & 0.665$\pm$0.131
% & & NotBeAloneTonight & 0.587$\pm$0.131
% & & Skiing & 0.766$\pm$0.104
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & StarWars & 0.950$\pm$0.016
% & & StarryPolar & 0.495$\pm$0.152
% & & Symphony & 0.779$\pm$0.096
% & & Surfing & 0.830$\pm$0.096
% \\
%  \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
%  & Terminator & 0.843$\pm$0.078
% & & WesternSichuan & 0.667$\pm$0.138
% & & VRBasketball & 0.770$\pm$0.105
% & & Waterskiing & 0.781$\pm$0.128
% \\
%  \hline
%  \multicolumn{2}{|c|}{Overall} & \multicolumn{10}{c|}{0.745$\pm$ 0.114}\\
%  \hline
%  \end{tabular}}
%\end{center}
%\end{table*}

%TABLE HA_HB_NEW
\begin{table*}
\begin{center}
  \caption{CC between ground-truth HM maps of Groups $A$ and $B$, for each panoramic video sequence} \label{tab:CC}
  \tiny
  \resizebox{\textwidth}{!}{
  \begin{tabular}{*{4}{|c|c|c}|}
  \hline
  \tabincell{c}{Cate-\\gory} & Name & CC & \tabincell{c}{Cate-\\gory} & Name & CC & \tabincell{c}{Cate-\\gory} & Name & CC & \tabincell{c}{Cate-\\gory} & Name & CC \\
  \hline
  \multirow{10}{*}{\rotatebox{90}{CA}} & AcerEngine & 0.891$\pm$0.058
&
  \multirow{10}{*}{\rotatebox{90}{Others}} & A380 & 0.867$\pm$0.058
 &
  \multirow{10}{*}{\rotatebox{90}{Driving}} & AirShow & 0.883$\pm$0.033
 &
  \multirow{10}{*}{\rotatebox{90}{Show}} & BTSRun & 0.926$\pm$0.041
\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & AcerPredator & 0.876$\pm$0.063

& & Camping & 0.851$\pm$0.073

& & Bicycle & 0.953$\pm$0.021

& & Catwalks & 0.834$\pm$0.069

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & BFG & 0.732$\pm$0.101

& & CandyCarnival & 0.830$\pm$0.088

& & DrivingInAlps & 0.858$\pm$0.073

& & DanceInTurn & 0.934$\pm$0.030

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & CMLauncher & 0.894$\pm$0.041

& & Lion & 0.937$\pm$0.034

& & F5Fighter & 0.572$\pm$0.109

& & Dancing & 0.906$\pm$0.052

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & CMLauncher2 & 0.924$\pm$0.045

& & MercedesBenz & 0.685$\pm$0.137

& & HondaF1 & 0.944$\pm$0.020

& & Graffiti & 0.906$\pm$0.037

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & LoopUniverse & 0.881$\pm$0.044

& & RingMan & 0.923$\pm$0.032

& & InsideCar & 0.903$\pm$0.036

& & Guitar & 0.866$\pm$0.096

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & Orion & 0.810$\pm$0.082

& & Shark & 0.812$\pm$0.116

& & Rally & 0.925$\pm$0.025

& & KasabianLive & 0.819$\pm$0.087

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & Pearl & 0.838$\pm$0.120

& & Square & 0.842$\pm$0.076

& & Stratosphere & 0.792$\pm$0.090

& & NotBeAloneTonight & 0.695$\pm$0.104

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & Roma & 0.896$\pm$0.054

& & TalkingInCar & 0.858$\pm$0.078

& & VRBasketball & 0.739$\pm$0.098

& & Symphony & 0.826$\pm$0.068
 \\
  \hline
 \multirow{10}{*}{\rotatebox{90}{VideoGame}} & Yacht & 0.780$\pm$0.126
&
 \multirow{10}{*}{\rotatebox{90}{Movie}} & IRobot & 0.907$\pm$0.029
&
 \multirow{10}{*}{\rotatebox{90}{Scenery}} & BlueWorld & 0.793$\pm$0.159
&
 \multirow{10}{*}{\rotatebox{90}{ActionSports}} & Supercar & 0.915$\pm$0.041
\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & WaitingForLove & 0.867$\pm$0.083

& & KingKong & 0.740$\pm$0.091

& & Castle & 0.698$\pm$0.105

& & Flight & 0.834$\pm$0.072

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & CS & 0.872$\pm$0.061

& & Murder & 0.921$\pm$0.058

& & Egypt & 0.779$\pm$0.094

& & Motorbike & 0.746$\pm$0.105

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & GalaxyOnFire & 0.895$\pm$0.062

& & Predator & 0.743$\pm$0.132

& & Manhattan & 0.795$\pm$0.085

& & Parachuting & 0.777$\pm$0.103

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & GTA & 0.930$\pm$0.026

& & ProjectSoul & 0.946$\pm$0.024

& & Snowfield & 0.824$\pm$0.142

& & Parasailing & 0.836$\pm$0.058

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & MC & 0.728$\pm$0.124

& & StarWars & 0.910$\pm$0.038

& & StarryPolar & 0.637$\pm$0.120

& & Skiing & 0.748$\pm$0.099

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & SpaceWar & 0.681$\pm$0.116

& & StarWars2 & 0.912$\pm$0.035

& & Sunset & 0.746$\pm$0.080

& & SnowRopeway & 0.728$\pm$0.093

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & SpaceWar2 & 0.577$\pm$0.148

& & Terminator & 0.908$\pm$0.054

& & Village & 0.819$\pm$0.084

& & Surfing & 0.850$\pm$0.080

\\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}

 & StreetFighter & 0.930$\pm$0.023

& & TheInvisible & 0.871$\pm$0.052

& & Waterfall & 0.824$\pm$0.066

& & SurfingArctic & 0.695$\pm$0.119
 \\
  \hline
  \multicolumn{2}{|c|}{Overall} & \multicolumn{10}{c|}{0.830$\pm$ 0.075}\\
  \hline
  \end{tabular}}
\end{center}
\end{table*}


\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/database/direction-consistence-distribution}}%of
		\caption{\footnotesize{Distribution of HM scan-path over 8 discrete directions, for all 76 sequences from the PVS-HM database. Note that the proportions of subjects, whose HM scan-path directions fall into each discrete direction, are ranked and then shown in this figure.}}
		\label{direction-consistence-distribution}
	\end{center}
\end{figure*}


\emph{\\Finding 1: When watching panoramic video, different subjects are highly consistent in HM positions.}
\\ \textit{Analysis:} In our PVS-HM database, we randomly divide all 58 subjects into two equal-size groups, $A$ and $B$.
For each frame of 76 sequences, the ground-truth HM maps of Groups $A$ and $B$ are generated by convolving with a 2D Gaussian filter over the collected HM data, along with longitude and latitude.
They are denoted as $H_A$ and $H_B$, respectively.
For a panoramic frame, we quantify the correlation of the HM maps between $H_A$ and $H_B$ using linear correlation coefficient (CC) \cite{li2015data}.
Table~\ref{tab:CC} lists the averaged CC ($\pm$ standard deviations) of HM maps between Groups $A$ and $B$, over all frames for each sequence.
We further show in Table Table~\ref{tab:CC} the CC values between $H_A$ and the maps with Gaussian distribution, uniform distribution and FCB.
It can be seen from this table that the CC values are rather high across different sequences.
It can be also seen from this table that the average CC value between $H_A$ and $H_B$ over all 76 panoramic sequences is XXX, with the standard deviation being XXX.
This CC value is significantly larger than that between $H_A$  between $H_A$ and random maps or FCB maps.
Thus, it is obvious that HM positions of subjects are highly consistent.
This completes the analysis of \textit{Finding 1}.

\emph{Finding 2: The magnitude of HM scan-paths is similar across subjects in viewing the same panoramic video.}
\\ \textit{Analysis:} Human HM scan-paths can be decomposed into magnitude and direction.
 Here, we measure the magnitude of HM scan-paths across different subjects.
For each individual sequence in our PVS-HM database, Figure \ref{consistence-magnitude} plots the mean and standard deviation of HM scan-path magnitude across all 58 subjects.
The last column of Figure \ref{consistence-magnitude} further shows the mean and standard deviation, averaged over all 76 sequences.
We can see from this figure that the standard deviation is much less than the mean value, for all 76 sequences and the averaged results.
Thus, we can conclude that there exists similarity for the magnitude of HM scan-paths across subjects, when the subjects view the same panoramic sequence.
This completes the analysis of \textit{Finding 2}.

 %rom this figure, we can see that the standard deviation of $22.7$ degree per second is far less than the mean of $47.7$ degree per second, for scan-path magnitudes of HM from different subjects.


\emph{Finding 3: The direction of HM scan-paths on panoramic video is with high consistency across subjects.}
\\ \textit{Analysis:} In  our PVS-HM database, we evaluate the consistency of the HM scan-path direction among all 58 subjects.
Specifically, we evaluate the direction of the scan-path starting from consistent HM regions (i.e., regions with similar HM positions), since \textit{Finding 1} has shown that the HM positions of different subjects are highly consistent. In our PVS-HM database, the consistent regions are extracted from 76 panoramic video sequences, which has HM positions of at least 12 subjects within a small \textit{great-circle distance} range of $3^{\circ}$ \cite{matin1974saccadic}. For each panoramic video sequence, Figure \ref{direction-consistence} shows the circular standard deviation \cite{frederic2010mean} of HM scan-path directions, starting from the consistent HM regions within one-second time slot \footnote{We have conducted our experiment with time slot being $0.1$, $1$ and $2$ seconds, and the results are similar.}. The last column of Figure \ref{direction-consistence} also reports the circular standard deviation averaged over all 76 sequences.
We can see from this figure that the circular standard deviation of the HM scan-path direction is averagely $38.1^{\circ}$, and it is significantly less than $103.9^{\circ}$ of randomly generated HM scan-path directions. Besides, the HM scan-path directions of all 76 sequences have considerably smaller circular standard deviation, compared to the random scan-paths.
This implies that there exists high consistency on directions of HM scan-paths across subjects.
Consequently, \textit{Finding 3} can be validated.



\emph{Finding 4: Almost $50\%$ subjects are consistent with one HM scan-path direction (among 8 uniform-quantized directions), and over $85\%$ subjects are consistent with three directions for HM scan-paths. }
\\ \textit{Analysis:} The distribution of HM scan-path directions in the PVS-HM database is analyzed as follows.
Only HM scan-paths falling into consistent HM regions (mentioned in \textit{Finding 3}) are selected for the analysis.
Specifically, we discretize continuous $0-360^{\circ}$ directions of HM scan-paths by 8-level uniform quantization: $\{0^{\circ}, 45^{\circ}, 90^{\circ} \cdots, 315^{\circ} \}$.
Then, we count the proportions of subjects, whose HM scan-paths belong to the same discretized direction. Next, such proportions are ranked by their values in each extracted HM region, i.e., ranking from the 1-st to 8-th. For each ranking, the proportions of subjects are averaged over all HM regions for each panoramic sequence, which are shown in Figure \ref{direction-consistence-distribution}.
We can see from this figure that for all 76 sequences, the HM scan-paths of $50\%$ subjects or more are consistent in the first ranked direction, and the HM scan-paths of over $85\%$ subjects are consistent in top 3 directions. We further show in Figure \ref{direction-consistence-pie} the proportions of subjects averaged over all 76 sequences, the HM scan-path directions of which rank 1-st to 8-th.
As seen in this figure, the directions of HM scan-paths from $57.1\%$, $20.6\%$ and $9.6\%$ subjects belong to the top 3 ranked directions. In contrast, the HM scan-path directions of $12.7\%$ subjects fall into other 5 directions. Therefore, \textit{Finding 4} can be validated.

\begin{figure}
	\begin{center}
		\centerline{\includegraphics[width=.6\columnwidth]{figures/database/direction-consistence-distribution-pie}}%of
		\caption{\footnotesize{Proportions of subjects with scan-path falling into each of 8 ranked directions. The proportions are averaged over all 76 sequences in the PVS-HM database. }}
		\label{direction-consistence-pie}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\centerline{\includegraphics[width=1\columnwidth]{figures/database/consi_on_time}}%of
		\caption{\footnotesize{Correlation coefficient of HM scan-paths over different time intervals and its variance. }}
		\label{consi_on_time}
	\end{center}
\end{figure}

\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=1.5\columnwidth]{figures/dhp_approach/main_framework}}%of
		\caption{Framework of our Offline-DHP approach.}
		\label{main-framework}
	\end{center}
\end{figure*}

\begin{table*}
\center
\caption{Notations denoted in Figure \ref{main-framework}} \label{notation_framework}
\begin{tabular}{ll}
%$\{t\}_{1}^{T}$ & The frames of panoramic video from $1$ to $T$, with $t$ being the currently processed frame \\
$\{\mathbf{F}_t\}_{t=1}^{T}$ & The panoramic frames with frame number $t$ ranging from $1$ to $T$, as the input to offline-DHP\\
$\{\mathbf{H}_t\}_{t=1}^{T}$ & The HM maps for frames from $1$ to $T$, as the output of offline-DHP\\
$\{m\}_{m=1}^{M}$ & The subjects from $1$ to $M$, with $m$ being the $m$-th subject\\
$\{n\}_{n=1}^{N}$ & The DRL workflows from $1$ to $N$, with $n$ being the $n$-th workflow\\
$\{\mathbf{o}^n_t\}_{t=1}^{T}$ & The FoV for frames from $1$ to $T$, as the $observation$ of the $n$-th DRL workflow \\
$(x^m_t, y^m_t)$ & The ground-truth HM position of the $m$-th subject at frame $t$ \\
$(\hat{x}^n_t, \hat{y}^n_t)$ & The HM position predicted by the $n$-th DRL workflow at frame $t$  \\
%$(\mathbf{x}_t, \mathbf{y}_t)$ & The predicted HM position at frame $t$ \\
$\mathbf{\pi}_t$ & The predicted probability distribution of HM direction at frame $t$, as the $policy$ of DRL \\
$\hat{\alpha}^n_t$ & The predicted HM scan-path direction at frame $t$ from the $n$-th DRL workflow, as the $action$ of DRL\\
$\alpha^m_t$ & The ground-truth HM scan-path direction of the $m$-th subject at frame $t$ \\
$\hat{\nu}^n_t$ & The predicted HM scan-path magnitude at frame $t$ from the $n$-th DRL workflow, as the $action$ of DRL\\
$\nu^m_t$ & The ground-truth HM scan-path magnitude of the $m$-th subject at frame $t$ \\
$r^{\alpha}_{n,t}$ & The \textit{reward} for deciding $\hat{\alpha}^n_t$ in the $n$-th DRL workflow\\
$r^{\nu}_{n,t}$ & The \textit{reward} for deciding $\hat{\nu}^n_t$ in  the $n$-th DRL workflow\\
$\mathbf{f}^n_{t}$ & The extracted LSTM feature at frame $t$,  as part of \textit{observed state} in the $n$-th DRL workflow \\
\end{tabular}
\end{table*}

\emph{Finding 5: For one subject, the HM scan-path at the current time interval is correlated with that at the previous time interval and observed video content. }
\\ \textit{Analysis:} For each individual subject, the vectors of HM scan-paths across the time interval of 0.3 second are extracted from all 76 sequences in our the PVS-HM database. Here, we calculate the correlation coefficient (CC) of HM scan-path vectors between two successive time periods, and then average the CC value over all 76 sequences for each subject. Figure \ref{consi_on_time} shows averaged CC values for each of 58 subjects, which range from 0.39 - 0.51. This figure implies that the HM scan-path at the current time interval is somewhat correlated with that at the previous time interval. On the other hand, \cite{hu2017deep} shows that the current HM scan-path of one subject is also related to observed video content. The above completes the validation of \textit{Finding 5}.

\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/database/huoshantu}}%of
		\caption{\footnotesize{The distribution of people's concerns on panorama video. }}
		\label{huoshantu}
	\end{center}
\end{figure*}

{\color{red}
\emph{Finding 6: When watching panorama video, people will be more concerned about the middle of the region, and people will tend to look around instead of looking up and down, which provides a basis for our prediction of the concerns. }

Analysis: We divide the entire panorama video into a $60$ x $30$ grid, and we counted the number of people's concerns on each grid.Then we got the following figure\ref{huoshantu} by fitting.

In this figure, the x axis represents the longitude, the y axis represents the dimension, and the z axis represents the number of views.
From \ref{huoshantu} we can see that this function is whit a single peak in the middle,which means that the overwhelming majority of people are focused on the middle, and this is our first conclusion. Then we can see that the protruding part of the figure is long in the longitude direction and short in the latitude direction, which means the number of people who watch in the direction of the longitude is more than people who watch in the direction of the latitude. This also illustrates our second conclusion,people will tend to look around instead of looking up and down.
This completes the verification of \textit{Finding 6}.
}


%The relevance decreases when the time interval is increased.
%\emph{Finding 5: HM scan-path is predictable.The behavior of the next moment is of great relevance to the state of the last moment.We have proved that correlation between the direction of the next moment and the direction of this moment is more than $87.9\%$ (The longer the time interval, the smaller the correlation will be). }
%\\ \textit{Analysis:} We used the time interval as an independent variable,calculated the correlation between t and t-1,t-2... until t-10.The results are as follows.
%From \ref{consi_on_time} we can see that the first-order correlation of the HM scan-paths direction is $87.9\%$.This indicates that the HM scan-paths direction depends largely on the direction of the last moment.That is , each HM scan-paths direction is based on the HM scan-paths direction of the last moment.And the correlation coefficient decreases with the increase of order, indicating that the correlation will decrease with the time interval increase.
%As seen in this figure, First order correlation coefficient up to $87.9\%$,while the variance is only 0.023.



\section{Offline-DHP approach}\label{sec::offline-DHP}

\subsection{Framework of offline-DHP}
\label{framework}

In this section, we present our offline-DHP approach, in light of our findings in Section \ref{Database_analysis}.
Figure \ref{main-framework} shows the overall framework of our approach, in which the multiple DRL workflows are embedded to generate HM maps of the input panoramic video frames. The notations used in this figure and in our approach are listed in Table \ref{notation_framework}.





%$\mathbf{f}_{t-1}$ & LSTM feature produced by DRL model at frame $t-1$. $\mathbf{F}_t$ and $f_{t-1}$ compose a \textit{full observed state} of DRL $agent$ \\

\begin{figure}
	\begin{center}
		\centerline{\includegraphics[width=1\columnwidth]{figures/dhp_approach/training_framework}}%of
		\caption{\footnotesize{Framework of training the DRL model for obtain each DRL workflow in Figure \ref{main-framework}.}}
		\label{train-framework}
	\end{center}
\end{figure}


As shown in Figure \ref{main-framework}, the input to our offline-DHP approach is the panoramic video frames $\{\mathbf{F}_t\}_1^{T}$.
Since \textit{Finding 1} has shown that the HM positions are highly consistent across different subjects,  we propose to generate HM maps for modeling human attention on the panoramic video, seen as the output of our offline-DHP approach. The HM map $\mathbf{H}_t$ of frame $t$ represents the probability of each pixel being the HM position.
Similar to saliency maps of 2D videos, $\mathbf{H}_t$ is obtained by convoluting the predicted HM positions $\{(\hat{x}^n_t, \hat{y}^n_t)\}_{n=1}^{N}$ with a 2D Gaussian filter.
Because \textit{Finding 4} has found that the HM scan-paths of different subjects are consistent in more than one directions, the HM positions $\{({x}^m_t, {y}^m_t)\}_{m=1}^{M}$ of $M$ subjects may be different from each other. Accordingly, this paper assumes that the number of the predicted HM positions $N$ is equivalent to $M$ at each frame, for predicting HM positions of all subjects.
In other words, to obtain $(\hat{x}^n_t, \hat{y}^n_t)$, our offline-DHP approach estimates the HM positions of one subject through HM scan-path prediction of one DRL workflow.
Then, $N$ DRL workflows are run to obtain $N$ HM positions $\{(\hat{x}^n_t, \hat{y}^n_t)\}_{n=1}^{N}$ at frame $t$, corresponding to the ground-truth HM positions  $\{({x}^m_t, {y}^m_t)\}_{m=1}^{M}$ of $M$ ($=N$) subjects at this frame.
Each of DRL workflows works independently to generate a HM scan-path by randomly sampling actions based on a learnt policy $\pi_t$.
Note that all DRL workflows share the same policy $\pi_t$ in our approach.

In a single DRL workflow , $\{(\hat{x}^n_t, \hat{y}^n_t)\}_{t=1}^T$ can be modeled by determining a series of \textit{actions}: $\{\hat{\alpha}^n_t\}_{t=1}^T$ and $\{\hat{\nu}^n_t\}_{t=1}^T$.
It is worth pointing out that $\{\hat{\alpha}^n_t\}_{t=1}^{T}$ and $\{\hat{\nu}^n_t\}_{t=1}^{T}$, as the \textit{actions} in the DRL workflow, are predicable, since \textit{Findings 2} and \textit{3} have indicated that subjects are consistent in the magnitudes and directions of HM scan-paths.
As can be seen in Figure \ref{main-framework}, in each workflow, one HM scan-path is generated through the interaction between FoV extractor and HM scan-path predictor.
Note that the extracted FoV is $103^{\circ} \times 60^{\circ}$, the same as the setting of the HMD.
Specifically, Figure \ref{main-framework} shows that FoV $\mathbf{o}^n_t$ is extracted via making its center locate at the HM position $(\hat{x}^n_t,\hat{y}^n_t)$, and $(\hat{x}^n_t,\hat{y}^n_t)$ is generated by the predicted \textit{action} of HM scan-paths $(\hat{\alpha}^n_{t-1},\hat{\nu}^n_{t-1})$ at the previous video frame.
Then, the content of extracted FoV works as \textit{observation} of DRL, for predicting the next \textit{action} of HM scan-path $(\hat{\alpha}^n_{t},\hat{\nu}^n_{t})$.
The HM scan-path generated by each DRL workflow is forwarded to obtain HM positions at incoming frames.
Afterwards, the HM positions from multiple DRL workflows are integrated, and then smoothed by the 2D Gaussian filter.
Finally, the HM maps $\{\mathbf{H}_t\}_1^{T}$ of the panoramic video are obtained, which model the heat maps for HM positions at each frame.

%Assuming that $p^n(x_t,y_t)$ is the probability of the $n$-th subject's HM position being at $(x_t,y_t)$, $p(x_t,y_t)$ can be estimated by the following expectation:
% \begin{equation}\label{define-hm-maps}
%   p(x_t,y_t) = \mathbb{E}\{ p^n(x_t, y_t) \}.
% \end{equation}
%
%To obtain $ p^n(x_t, y_t)$, our offline-DHP approach estimates the HM positions of one subject through HM scan-path prediction of one DRL workflow. In fact, $p^n(x_t, y_t)$ of \eqref{define-hm-maps} can be modeled by determining a series of \textit{actions} in the DRL workflow: $\alpha_1,\alpha_2,...,\alpha_t$ and $\nu_1,\nu_2,...,\nu_t$. It is worth pointing out that it is reasonable to model $\{\alpha_t\}_{t=1}^{T}$ and $\{\nu_t\}_{t=1}^{T}$ as the \textit{actions} in the DRL workflow, since \textit{Findings 2} and \textit{3} have indicated that subjects are consistent in the magnitudes and directions of HM scan-paths.

%Since \textit{Finding 4} has found that the HM scan-paths of different subjects are consistent in more than one directions, it is impossible to model $p(x_t,y_t)$ of \eqref{define-hm-maps} through $p^n(x_t, y_t)$ of only one DRL workflow.
%Instead, the HM map can be generated by running $N$ DRL workflows to calculate $p(x_t,y_t)$ with \eqref{define-hm-maps}.
%More specifically, our offline-DHP method runs multiple DRL workflows, each of which works independently to generate a HM scan-path by random sampling actions based on a learnt policy $\pi_t$.
%The generated HM scan-paths of all DRL workflows are then integrated to produce the HM map upon the expectation of HM positions in \eqref{define-hm-maps}.



%\begin{proposition}
%    \label{lemma2}
%      $\mathbb{E}$ in \eqref{define-hm-maps} can be modeled by running multiple DRL workflows, given a learned HM scan-path predictor.
%        \begin{eqnarray}
%        \label{proposition_2}
%        && \mathbb{E}\{ p^n(x_t, y_t) \} \nonumber\\
%        &=& \mathbb{E}_{\pi(x_{t},y_{t} |\Psi_{ x_{t-1}, y_{t-1} } ),...,\pi(x_{2},y_{2} |\Psi_{ x_{1}, y_{1} } )} \{ \mathbb{E} \{ p^n( \Psi_{ x_{1}, y_{1} } ) \} \}.
%        \end{eqnarray}
%
%    \textbf{Proof:}
%        Since \textit{Findings 2-4} have revealed that $\alpha^n_t$ and $\nu^n_t$ are generally consistent across different $n$, we assume that $p(\alpha_{t-1}, \nu_{t-1} | \Psi_{ x_{t-1}, y_{t-1} } )$ of all subjects is with the same probability distribution, denoted by $\pi(x_{t},y_{t}|\Psi_{x_{t-1}, y_{t-1} } )$. That is,
%        \begin{eqnarray}
%        \label{p-sim-pi}
%        p( \alpha_{t-1}, \nu_{t-1} | \Psi_{ x_{t-1}, y_{t-1} } ) \sim \pi(x_{t},y_{t} | \Psi_{ x_{t-1}, y_{t-1} } ).
%        \end{eqnarray}
%        Thus, based on \eqref{expand-define-with-full-prob-formula} and \eqref{p-sim-pi}, the expectation of \eqref{use-define-of-expectation} can be rewritten in the following,
%        \begin{eqnarray}
%        \label{use-define-of-expectation1}
%        && \mathbb{E}\{ p^n(x_t, y_t) \} \nonumber\\
%        \nonumber &=& \mathbb{E}\{ p^n(\Psi_{ x_t, y_t }) \} \nonumber\\
%        &=& \mathbb{E}_{\pi(x_{t},y_{t} |\Psi_{ x_{t-1}, y_{t-1} } ) } \{ \mathbb{E} \{ p^n( \Psi_{ x_{t-1}, y_{t-1} } ) \} \}.
%        \end{eqnarray}
%         Above \eqref{use-define-of-expectation1} can be iterated till $p^n( \Psi_{ x_{1}, y_{1} } )$, which derives \eqref{proposition_2} in Proposition 2.
%
%\end{proposition}


\subsection{DRL model of the offline-DHP approach}
\label{train}
As described in Sections \ref{framework}, the DRL workflow is a key component in our offline-DHP framework, which targets at predicting HM scan-paths.
This section presents how to train the DRL model of each workflow in Figure \ref{main-framework} for predicting HM maps.
In this section, we take the $n$-th workflow as an example.
Figure \ref{train-framework} shows the framework of training the DRL model.
As shown in Figure \ref{train-framework}, the FoV of input video frame is extracted upon the \textit{action} of the predicted HM scan-path at the previous frame.
The extracted FoV, as the \textit{observation}, is then feeded into the DRL network.
In addition, the \textit{reward}, which measures the similarity between the prediction and ground-truth of HM scan-paths, is estimated to evaluate the \textit{action} made by the DRL model. Then, the \textit{reward} is used to make decision on the \textit{action} through the DRL model, i.e., the HM scan-path at the current frame. In the DRL model, the \textit{observation} of the extracted FoV and the \textit{reward} of HM scan-path prediction compose an \textit{environment}.

In training the DRL model,  \textit{environment} interacts with the HM scan-path predictor.
The interaction is achieved in our DRL model by the following procedure.\\
(1) At frame $t$, the FoV extractor obtains the current $\textit{observation}$ $\mathbf{o}^n_t$ ($103^{\circ} \times 60^{\circ}$) from the input video frame $\mathbf{F}_t$, according to the predicted HM position $(\hat{x}^n_t,\hat{y}^n_t)$.
In our work, $\mathbf{o}^n_{t}$ is projected to the 2D region and then down-sampled to $42\times42$.\\
(2) The current $\mathbf{o}^n_t$ and the LSTM feature $\mathbf{f}^n_{t-1}$ from  the last frame are delivered to the DRL network in the HM scan-path predictor.
In our work, the DRL network contains four convolutional layers \cite{simonyan2014very} and one LSTM layer \cite{hochreiter1997long, hausknecht2015deep}, which are used to extract the spatial and temporal features, respectively. The details about the architecture of the DRL network can be found in Figure \ref{train-framework}.\\
(3) At frame $t$, the DRL network produces the LSTM feature $\mathbf{f}^n_{t}$, HM scan-path magnitude $\hat{\nu}^n_{t}$ and policy $\pi_{t}$. Here, $\pi_{t}$ is the probability distribution over the \textit{actions} of HM scan-path directions.\\
(4) Given $\pi_{t}$, the HM scan-path predictor randomly samples an \textit{action} $\hat{\alpha}^n_t$, with standard deviation $\varepsilon$ to ensure exploration. Here, $\hat{\alpha}^n_t$ includes 8 discrete directions in GDS: $\{ 0^{\circ}, 45^{\circ}, \cdots, 315^{\circ} \}$.\\
(5) \textit{Environment} is updated with $\hat{\nu}^n_t$ and $\hat{\alpha}^n_t$, leading to $(\hat{x}^n_t, \hat{y}^n_t)\longrightarrow (\hat{x}^n_{t+1},\hat{y}^n_{t+1})$. The FoV extractor returns a new $\mathbf{o}^n_{t+1}$ according to the HM position $(\hat{x}^n_{t+1},\hat{y}^n_{t+1})$. The \textit{reward} estimator returns the \textit{rewards} $r^{\nu}_{n,t}$ and $r^{\alpha}_{n,t}$ in predicting $\hat{\nu}^n_t$ and $\hat{\alpha}^n_t$, upon the ground-truth HM scan-paths of $\{\nu^m_t\}_{m=1}^{M}$ and $\{\alpha^m_t\}_{m=1}^{M}$. \\
(6) A set of experiences $\{ \mathbf{o}^n_{t}, \! \mathbf{f}^n_{t-1},\! \hat{\nu}^n_t,\! \hat{\alpha}^n_t,\! r^{\nu}_{n,t},\! r^{\alpha}_{n,t} \}$ are stored in an experience buffer for frame $t$.
In addition, $\mathbf{o}^n_{t+1}$ and $\mathbf{f}^n_{t}$ are preserved for processing frame $t+1$.\\
(7) Once $t$ meets the terminal condition of exceeding the frame number $T$, all experiences in the buffer are delivered to the optimizer for updating the DRL network.



\textbf{Reward Estimation.}
Next, we focus on modeling the \textit{rewards}: $r^{\alpha}_{n,t}$ and $r^{\nu}_{n,t}$.
When training the DRL model, our goal is to make $\hat{\alpha}^n_{t}$ and $\hat{\nu}^n_{t}$ approach to the ground-truth HM data.
Thus, the  \textit{rewards} $r^{\alpha}_{n,t}$ and $r^{\nu}_{n,t}$ can be represented by the differences from $\hat{\alpha}^n_{t}$ and $\hat{\nu}^n_{t}$ to $\{{\alpha}^m_{t}\}_{m=1}^M$ and $\{{\nu}^m_{t}\}_{m=1}^M$, respectively.
In our approach, we use the Gaussian distribution to measure the differences between $\hat{\alpha}^n_{t}$ and the ground-truth $\{\alpha^{m}_{t}\}_{m=1}^M$. In addition, we also consider the distances from predicted HM position $(\hat{x}^n_t,\hat{y}^n_t)$ to $\{(x^{n}_{t},y^{n}_{t})\}_{m=1}^M$ in calculating the \textit{rewards} of $r^{\alpha}_{n,t}$ and $r^{\nu}_{n,t}$. Such distances are modeled by the 2D Gaussian distribution.
Then, $r^{\alpha}_{n,t}$ can be written as
\begin{equation}
\label{reward-alpha}
r^{\alpha}_{n,t} = \frac{1}{N}\sum_{m=1}^{M} e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}^n_{t}, \alpha^m_{t})}{\rho}\right)^2} e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}^n_{t},\hat{y}^n_{t}),(x^m_{t},y^m_{t}))}{\varrho}\right)^2},
\end{equation}
In \eqref{reward-alpha}, $D_d$ defines the \textit{phase difference}, and $D_s$ denotes the \textit{great-circle distance} \cite{shumaker1984astronomical}. Moreover, $\rho$ and $\varrho$ are the standard deviations of Gaussian distributions, as the hyper-parameters. Similarly, we have
\begin{equation}
\label{reward-nu}
r^{\nu}_{n,t} \!\!=\!\! \frac{1}{N}\!\sum_{m=1}^{M}\!
e^{\!-\frac{1}{2}\left(\!{\frac{\hat{\nu}^n_{t}-\nu^{m}_{t}}{\varsigma}}\!\right)^2} \! e^{\!-\frac{1}{2}\left(\!\frac{D_d(\hat{\alpha}^n_{t}, \alpha^m_{t})}{\rho}\!\right)^2} \!\!e^{\!-\frac{1}{2}\left(\!\frac{D_s((\hat{x}^n_{t},\hat{y}^n_{t}),(x^m_{t},y^m_{t}))}{\varrho}\!\right)^2}.
\end{equation}
where $\varsigma$ is the hyper-parameter for the standard deviation of HM scan-path magnitude.


%\begin{proposition}
%    \label{lemma3}
%    Assume that probability $p(x_t,y_t)$ decays along with the distance of $(x_t,y_t)$ to the ground-truth HM position $(x^{n}_{t},y^{n}_{t})$, obeying the 2D Gaussian distribution.
%    Additionally, assume that the probability of $\alpha_{t}$ also follows the Gaussian distribution with the mean being $\alpha^{n}_{t}$.
%    Then, $r^{\alpha}_t$ can be represented by \eqref{reward-alpha}.
%
%    \textbf{Proof:} At frame $t$, the ground-truth HM position is $\{x^{n}_{t},y^{n}_{t} \}$ for  the $n$-th subject.
%    Besides, $\alpha^{n}_{t}$ is the  ground-truth direction of HM scan-path for the $n$-th subject.
%    Thus, we can acquire:
%    \begin{eqnarray}
%        \label{reward-Pro10}
%        && P(\alpha^{n}_{t}|(x^{n}_{t}, y^{n}_{t})) = 1.
%        %\label{reward-Pro2}
%%        && P(\nu^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\},\alpha^{n}_{t}) = 1
%    \end{eqnarray}
%    Since the probability of $(x_{t},y_{t})$ being the HM position follows Gaussian distribution centered at $(x^{n}_{t},y^{n}_{t})$, the equality below can be obtained:
%    \begin{equation}
%        \label{reward-Pro11}
%        P_{n}(\alpha^{n}_{t}|(x_{t},y_{t})) = P_{n}(\alpha^{n}_{t}|(x^n_{t},y^n_{t})) e^{-\frac{1}{2}\left(\frac{D_s((x_{t},y_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}.
%    \end{equation}
%    Similarly, we have
%    \begin{eqnarray}
%        \label{reward-Pro12}
%        P_{n}(\alpha_{t}|(x_{t},y_{t})) = P_{n}(\alpha^{n}_{t}|(x_{t},y_{t})) e^{-\frac{1}{2}\left(\frac{D_d(\alpha_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%    Based on \eqref{reward-Pro11}, we can rewrite \eqref{reward-Pro12} as
%    \begin{eqnarray}
%        \label{reward-Pro5}
%        \nonumber && P_{n}(\alpha_{t}|(x_{t},y_{t})) = P_{n}(\alpha^{n}_{t}|(x^{n}_{t}, y^{n}_{t})) \cdot \\ && e^{-\frac{1}{2}\left(\frac{D_s((x_{t},y_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}
%        e^{-\frac{1}{2}\left(\frac{D_d(\alpha_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%   Because of \eqref{reward-Pro10}, the following holds:
%    \begin{equation}
%        \label{reward-Pro6}
%        P_{n}(\alpha_{t}|(x_{t},y_{t}))\!=\! e^{-\frac{1}{2}\left(\frac{D_d(\alpha_{t}, \alpha^n_{t})}{\rho}\right)^2}\! e^{-\frac{1}{2}\left(\frac{D_s((x_{t},y_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}.
%    \end{equation}
%    Reward $r^{\alpha}_t$ can be represented by $P_{n}(\alpha_{t}|(x_{t},y_{t}))$ averaged overall subjects, i.e., \eqref{reward-alpha}, which calculates how likely the subjects conduct the action of $\alpha_{t}$. Finally, this proposition is proved.
%\end{proposition}



%\begin{proposition}
%    \label{lemma3}
%    Assume that the probability of each pixel $\{\hat{x}_{t},\hat{y}_{t}\}$ being the HM position decays along with the distance to the ground truth HM position, following 2D coordinate Gaussian distribution \textcolor{blue}{along with longitude and latitude}.
%    In addition, assume that the probability for the degree $\alpha^{n}_{t}$ of HM scan path also follows the Gaussian distribution with the mean being $\alpha^{n}_{t}$.
%    Then $r^{\alpha}_t$ can be represented by \eqref{reward-alpha}.
%
%    \textbf{Proof:} At frame $t$, the ground truth HM position is $\{x^{n}_{t},y^{n}_{t} \}$ for  the $n$-th subject.
%    besides, $\alpha^{n}_{t}$ is the direction of HM scan path for the $n$-th subject.
%    Thus, we can acquire:
%    \begin{eqnarray}
%        \label{reward-Pro1}
%        && P(\alpha^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\}) = 1.
%        %\label{reward-Pro2}
%%        && P(\nu^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\},\alpha^{n}_{t}) = 1
%    \end{eqnarray}
%    Since the probability of $\{\hat{x}_{t},\hat{y}_{t}\}$ being the HM position follows Gaussian distribution centered at $\{x^{n}_{t},y^{n}_{t} \}$, the equality below can be obtained:
%    \begin{equation}
%        \label{reward-Pro3}
%        P_{n}(\alpha^{n}_{t}|\{\hat{x}_{t},\hat{y}_{t}\}) = P_{n}(\alpha^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\}) e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}_{t},\hat{y}_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}.
%    \end{equation}
%    Similarly, we have:
%    \begin{eqnarray}
%        \label{reward-Pro4}
%        P_{n}(\hat{\alpha}_{t}|\{\hat{x}_{t},\hat{y}_{t}\}) = P_{n}(\alpha^{n}_{t}|\{\hat{x}_{t},\hat{y}_{t}\}) e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%    Based on \eqref{reward-Pro3}, \eqref{reward-Pro4} can be written as:
%    \begin{eqnarray}
%        \label{reward-Pro5}
%        \nonumber && P_{n}(\hat{\alpha}_{t}|\{\hat{x}_{t},\hat{y}_{t}\}) = P_{n}(\alpha^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\}) \cdot \\ && e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}_{t},\hat{y}_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}
%        e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%    According to \eqref{reward-Pro1}, the following holds:
%    \begin{equation}
%        \label{reward-Pro6}
%        P_{n}(\hat{\alpha}_{t}|\{\hat{x}_{t},\hat{y}_{t}\}) = e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}_{t}, \alpha^n_{t})}{\rho}\right)^2} e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}_{t},\hat{y}_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}.
%    \end{equation}
%    Reward $r^{\alpha}_t$ can be represented by $P_{n}(\hat{\alpha}_{t}|\{\hat{x}_{t},\hat{y}_{t}\})$ averaged overall subjects, i.e., \eqref{reward-alpha},which estimates how likely human agent conducts the action of $\hat{\alpha}_{t}$. Finally, this proposition is proved.
%\end{proposition}

%\begin{proposition}
%    \label{lemma4}
%    Assume that the probability of each pixel $\{\hat{x}_{t},\hat{y}_{t}\}$ being the HM position decays along with the distance to the ground truth HM position, following 2D coordinate Gaussian distribution \textcolor{blue}{along with longitude and latitude}.
%    In addition, assume that the probability for the degree $\alpha^{n}_{t}$ of HM scan path also follows the Gaussian distribution with the mean being $\alpha^{n}_{t}$.
%    Then $r^{\alpha}_t$ can be represented by \eqref{reward-alpha}.
%    Provided that the probability for the magnitude $\hat{\nu}_{t}$ of HM scan path follows the Gaussian distribution with the mean of $\nu^{n}_{t}$.
%    We have \eqref{reward-nu}.
%
%    \textbf{Proof:} At frame $t$, $\nu^{n}_{t}$ is the magnitude of HM scan path at the direction $\alpha^{n}_{t}$ for the $n$-th subject.
%    Then, similar to the proof of \eqref{reward-Pro1}, the following holds:
%    \begin{equation}
%        \label{mu-Pro1}
%        P(\nu^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\},\alpha^{n}_{t}) = 1.
%    \end{equation}
%    \begin{eqnarray}
%        \label{mu-Pro2}
%        \nonumber && P_{n}(\nu^{n}_{t}|\{\hat{x}_{t},\hat{y}_{t}, \hat{\alpha}_{t}\}) = P(\nu^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\},\alpha^{n}_{t}) \cdot \\ && e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}_{t},\hat{y}_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}
%        e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%    Due to the Gaussian distribution of the probability for magnitude $\hat{\nu}_{t}$, we can obtain:
%    \begin{eqnarray}
%        \label{mu-Pro3}
%        \nonumber && P_{n}(\nu^{n}_{t}|\{\hat{x}_{t},\hat{y}_{t}, \hat{\alpha}_{t}\})\\
%        \nonumber &=& P(\nu^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\},\alpha^{n}_{t})
%        e^{-\frac{1}{2}\left({\hat{\nu}_{t}-\nu^{n}_{t}}\right)^2} \cdot \\ &&
%        e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}_{t},\hat{y}_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}
%        e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%    According to \eqref{mu-Pro1} and \eqref{mu-Pro3}, we have:
%    \begin{eqnarray}
%        \label{mu-Pro4}
%        \nonumber && P_{n}(\nu^{n}_{t}|\{\hat{x}_{t},\hat{y}_{t}, \hat{\alpha}_{t}\}) =
%        e^{-\frac{1}{2}\left({\hat{\nu}_{t}-\nu^{n}_{t}}\right)^2} \cdot \\ &&
%        e^{-\frac{1}{2}\left(\frac{D_s((\hat{x}_{t},\hat{y}_{t}),(x^n_{t},y^n_{t}))}{\varrho}\right)^2}
%        e^{-\frac{1}{2}\left(\frac{D_d(\hat{\alpha}_{t}, \alpha^n_{t})}{\rho}\right)^2}.
%    \end{eqnarray}
%    Reward $r^{\nu}_{t}$ can be similarly represented by
%    $P(\nu^{n}_{t}|\{x^{n}_{t}, y^{n}_{t}\},\alpha^{n}_{t})$ averaged overall subjects, i.e., \eqref{reward-nu}.
%    This completes the proof of proposition 4.
%\end{proposition}

\textbf{Optimization.}
Next, we need to optimize the \textit{rewards}: $r^{\alpha}_{n,t}$ and $r^{\nu}_{n,t}$, when learning the network parameters of our DRL model in Figure \ref{train-framework}.
Our offline-DHP approach implements the asynchronous DRL method \cite{mnih2016asynchronous} to learn the DRL parameters with optimized \textit{rewards}.
Hence, multiple workflows are run to interact with multiple \textit{environments} with workflow-specific parameter vectors $\{ \theta^{n}_{\nu}, \theta^{n}_{\pi}, \theta^{n}_{V} \}$, producing $\hat{\nu}^n_t$, $\hat{\pi}^n_t$ and $V$.
Here, $V$ denotes \textit{state value} output by the DRL network, which is obtained using the same way as \cite{mnih2016asynchronous}.
Meanwhile, global-shared parameter vectors $\{ \theta_{\nu}, \theta_{\pi}, \theta_{V} \}$\footnote{As can be seen in Figure \ref{train-framework}, $\{ \theta_{\nu}, \theta_{\pi}, \theta_{V} \}$ share all CNN and LSTM layers in our offline-DHP approach, but they are separated at the output layer.} are updated via accumulating gradient.
For more details about the workflow-specific and global-shared parameter vectors, refer to \cite{mnih2016asynchronous}.
In our approach, \textit{reward} $r^{\nu}_{n,t}$ is optimized to train $\theta_{\nu}$ as follows,
\begin{equation}
\label{opt-1}
d \theta_{\nu} \leftarrow d \theta_{\nu} + \nabla_{\theta_{\nu}^{n}} \sum_{t=1}^{T} r^{\nu}_{n,t}.
\end{equation}
Besides, we can optimize \textit{reward} $r^{\alpha}_{n,t}$ by
\begin{equation}
\label{opt-2}
\small d \theta_{V} \leftarrow d \theta_{V} + \nabla_{\theta_{V}^{n}} \sum_{t = 1}^{T} (\sum_{i=t}^{T} \gamma^{i-t} r^{\alpha}_{n,i} - V(\mathbf{o}^n_{t}, \mathbf{f}^n_{t-1} ; \theta_{V}^{n}))^2,
\end{equation}
\begin{small}
\begin{eqnarray}
\label{opt-3}
\hspace{0.1cm} \nonumber d \theta_{\pi} \leftarrow d \theta_{\pi} +\nabla_{\theta_{\pi}^{n}} \sum_{t = 1}^{T} \log \pi( \hat{\alpha}^n_{t} | \mathbf{o}^n_{t}, \mathbf{f}^n_{t-1} ; \theta_{\pi}^{n})\cdot \\
(\sum_{i = t}^{T} \gamma^{i-t} r^{\alpha}_{n,i} - V(\mathbf{o}^n_{t}, \mathbf{f}^n_{t-1} ; \theta_{V}^{n})),
\end{eqnarray}
\end{small}
where $\gamma$ is the discount factor of \textit{Q-learning} \cite{watkins1992q}.
In addition, $V(\mathbf{o}^n_{t}, \mathbf{f}^n_{t-1} ; \theta_{V}^{n})$ denotes state value $V$ yielded by $\mathbf{o}^n_{t}, \mathbf{f}^n_{t-1}$ and $\theta_{V}^{n}$; $\pi( \hat{\alpha}^n_{t} | \mathbf{o}^n_{t}, \mathbf{f}^n_{t-1} ; \theta_{\pi}^{n})$ stands for the probability of \textit{action} $\hat{\alpha}^n_{t}$ that is yielded by policy $\pi_t$ from $\mathbf{o}^n_{t}, \mathbf{f}^n_{t-1}$ and $\theta_{\pi}^{n}$.
Finally, upon the above equations, RMSProp \cite{tieleman2012lecture} is applied to optimize \textit{rewards} in training data. As a result, the workflow-specific and global-shared parameter vectors can be learned to predict HM scan-paths.


\section{Online-DHP approach}

\begin{figure}
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{figures/dhp_approach_on_line/whole_framework_online_5}}%of
		\caption{\footnotesize{Framework of our online-DHP approach.}}
		\label{online-framework}
	\end{center}
\end{figure}

In this section, we present our online-DHP approach.
The online-DHP approach refers to predicting the a specific subject's HM position $(\hat{x}_{t+1},\hat{y}_{t+1})$ at frame $t+1$, given his/her HM positions $\{(x_{1},y_{1}),\ldots, (x_{t},y_{t})\}$ till frame $t$.
Note that the definition of notations in this section is similar to those of Section \ref{sec::offline-DHP}, and the only difference is that $n$ is removed in all notations as there is only one subject/workflow in online-DHP.
In this section, we define the subject as the \textit{viewer}, whose HM positions need to be predicted online. % online viewer
Figure \ref{online-framework} shows the framework of our online-DHP approach. %, which predicts the HM position $(x_{t+1},y_{t+1})$ upon observation $\{\mathbf{o}_1, \ldots, \mathbf{o}_t\}$ till frame $t$.
According to \textit{Finding 5}, the input to our online-DHP framework is the \textit{viewer's} HM scan-path $\{(\alpha_1,\nu_1),\ldots, (\alpha_{t-1},\nu_{t-1})\}$ and frame content $\{\mathbf{F}_1, \ldots, \mathbf{F}_t \}$, while the output is the predicted HM position $(\hat{x}_{t+1},\hat{y}_{t+1})$ at the next frame for the \textit{viewer}.
This can be seen as online prediction of HM positions $\{(\hat{x}_{t},\hat{y}_{t})\}_{t=1}^{T}$ .
To this end, our online-DHP consists of two stages: the training and prediction stages.
At the first stage, the parameters of the DRL network are trained.
At the second state, the \textit{action} of HM scan-path is yielded upon the trained DRL network, to predict the HM position online.
In the following, we discuss these two stages with more details.

\subsection{Stage I: Training}
At the beginning frame, the HM position $(\hat{x}_1,\hat{y}_1)$ of the \textit{viewer} is initialized to be the center of front region, which is the general setting of  the panoramic video player. Then, the trained DRL network of our offline-DHP approach is loaded as the initial DRL network for online prediction, and both networks share the same structure.
The reason for loading the offline-DHP network is that it contains the knowledge of HM related features.
Later, this initial DRL network is fine-tuned by the \textit{viewer's} HM scan-path at incoming frames.

Next, we focus on the algorithm of training the DRL network in our online-DHP approach. As aforementioned, the initial parameters of the DRL network at the first frame are directly from those of offline-DHP. At each of the incoming frames, several episodes are run to update the DRL network of online-DHP. The following summarizes the procedure of one episode at frame $t+1$.

%As aforementioned, the offline DRL network is loaded in the initialization of our online-DHP approach. Thus, training the DRL network for online-DHP generally follows that of offline-DHP, both containing the basic reinforcement learning procedure. XXX.
%To predict the HM position of
%
%The following shows the procedure of one episode,
\begin{enumerate}
  \item Iterate the following steps from $i=1$ to $t$. At each iteration, $(\hat{\alpha}_i,\hat{\nu}_i)$ and $(\alpha_i,\nu_i)$ are the predicted and ground-truth \textit{actions} of the HM scan-path for the subject, and $\mathbf{o}_i$ is the \textit{observation} of the FoV content.
  \item Take \textit{action} of $(\hat{\alpha}_i,\hat{\nu}_i)$ using the DRL network, given the current \textit{observation} $\{\mathbf{o}_1, \ldots, \mathbf{o}_i\}$ till frame $i$. The \textit{action} of $\hat{\alpha}_i$ selects 1 among 8 discrete HM scan-path directions, i.e., $\{ 0^{\circ}, 45^{\circ}, \cdots, 315^{\circ} \}$. The \textit{action} of $\hat{\nu}_i$ also contains a scalar of HM scan-path magnitude.
  \item Calculate \textit{rewards} $(r^{\alpha}_{i}, r^{\nu}_{i})$ from the \textit{reward} estimator with \eqref{reward-alpha} and \eqref{reward-nu}, which measures how close the \textit{action} $(\hat{\alpha}_i,\hat{\nu}_i)$ to the ground-truth HM scan-path $(\alpha_i,\nu_i)$  of the \textit{viewer}. Here, the \textit{reward} calculation does not require the sums of \eqref{reward-alpha} and \eqref{reward-nu}, since the ground-truth HM scan-path of online prediction is from a single \textit{viewer}, rather than from all subjects.
  \item Generate new \textit{observation} ${\mathbf{o}_{i+1}}$ from the FoV extractor with the above \textit{action} $(\hat{\alpha}_i,\hat{\nu}_i)$, and then it is input to the DRL network.
  \item Update the DRL network using \eqref{opt-1}, \eqref{opt-2} and \eqref{opt-3}, if the iteration number $i$ is equivalent to $t$.
\end{enumerate}
Here,  the definitions of \textit{action}, \textit{reward} and \textit{observation} are the same as Section \ref{train}.
The above iterations share the same implementation of training the DRL model in offline-DHP, which is already presented in Section \ref{train}.
%The detailed implementation of reinforce learning has been presented in Section \ref{train}.

%Note that the definitions of \textit{action}, \textit{reward} and \textit{observation} are the similar to Section \ref{train}.
%The detailed implementation of the above process has been presented in Section \ref{train}.
%The main differences between the online- and offline-DHP approach are:
%
%\begin{itemize}
%    \item \textcolor{red}{For online-DHP, the termination condition of iteration is $t$, where $t$ is the frame the viewer is currently seeing, instead of the last frame of the video in offline-DHP.}
%    \item For online-DHP, the ground-truth HM scan-path used in \eqref{reward-alpha} and \eqref{reward-nu} is from the single viewer, rather then from all subjects.
%\end{itemize}

Once the above iterations are terminated, our algorithm moves to the next episode.
%The episodes end for frame $t+1$ and the final \textit{action} $(\hat{\alpha}^n_t,\hat{\nu}^n_t)$ is used to obtain the predicted HM position $(\hat{x}^n_{t+1},\hat{y}^n_{t+1})$ at this frame, when meeting the termination conditions.
After a number of episodes, the training stage ends for frame $t+1$, when meeting the the termination conditions.
In our approach, there are two criteria for the termination condition. The first criterion is the maximum number $E$ of episodes, which is set to be 500 in this paper.
The second one is based on the metric of mean overlap (MO), which measures how close the predicted HM position to the ground-truth HM position.
MO ranges from 0 to 1, and larger MO indicates more precise prediction.
Specifically, MO is defined as,
\begin{equation}
\label{mo-defination}
\textrm{MO} =\frac{A(\textrm{FoV}_{p} \cap \textrm{FoV}_{g})}{A(\textrm{FoV}_{p} \cup \textrm{FoV}_{g})},
\end{equation}
where $\textrm{FoV}_{p}$ and $\textrm{FoV}_{g}$ represent the FoVs at predicted and ground-truth HM positions, respectively.
In \eqref{mo-defination}, $A$ represents the area of a panoramic region, which counts for number of pixels.

Finally, the training DRL network can be obtained at frame $t+1$, once satisfying one of  the above termination criteria.
Algorithm \ref{online-DHP-algorithm-training} presents the summary of the training stage in online-DHP.



\begin{algorithm}
   \caption{The algorithm of the training stage in online-DHP for frame $t+1$.}
   \label{online-DHP-algorithm-training}
   \footnotesize
\begin{algorithmic}
   \STATE {\bfseries Input:} Panoramic video frames $\{\mathbf{F}_1, \ldots, \mathbf{F}_t \}$, and the ground-truth HM positions of the \textit{viewer} $\{(x_{1},y_{1}),\ldots, (x_{t},y_{t})\}$;
   \STATE Initialize the DRL network of online-DHP with parameter vectors $\{ \theta_{{\nu}}, \theta_{{\pi}}, \theta_{V} \}$, by loading the network of offline-DHP;
   \FOR{$e=1$ {\bfseries to} $E$}
       \STATE Initialize the HM position to be center of front region: $\hat{x}_1=0,\hat{y}_1=0$;
       \STATE Initialize LSTM feature to be the zero vector: $\mathbf{f}_0=\mathbf{0}$;
       \FOR{$i=1$ {\bfseries to} $t-1$}
           \STATE Extract \textit{observation} $\mathbf{o}_i$ (FoV) from $\mathbf{F}_i$ according to $(\hat{x}_{i},\hat{y}_{i})$;
           \STATE Obtain \textit{policy} $\pi_{i}$ and LSTM feature $\mathbf{f}_{i}$ using the DRL network with $\{\mathbf{o}_i, \mathbf{f}_{i-1}, \theta_{\pi}\}$;
           \STATE Select \textit{action} $\hat{\alpha}_{i}$ according to the $\epsilon$-greedy policy of $\pi_{i}$;
           \STATE Generate \textit{action} $\hat{\nu}_{i}$ using the DRL network given $\mathbf{o}_{i}, \mathbf{f}_{i-1}$ and $ \theta_{\nu}$;
           \STATE Calculate $(\hat{x}_{i+1}, \hat{y}_{i+1})$ with regard to $\hat{\alpha}_{i},\hat{\nu}_{i}$, and $(\hat{x}_{i}, \hat{y}_{i})$;
           \STATE Estimate \textit{rewards} $r^{\nu}_{i},\! r^{\alpha}_{i}$ of $\hat{\alpha}_{i},\hat{\nu}_{i}$ using \eqref{reward-alpha}, \eqref{reward-nu};
           \STATE Calculate MO between $(\hat{x}_{i},\hat{y}_{i})$ and $(x_{i},y_{i})$, denoted as $\text{MO}_i$;
           \STATE Store a set of experience: $\{ \mathbf{o}_{i}, \! \mathbf{f}_{i-1},\! \hat{\nu}_{i},\! \hat{\alpha}_{i},\! r^{\nu}_{i},\! r^{\alpha}_{i} \}$;
           \STATE $i \leftarrow i+1$;
       \ENDFOR
       \STATE Update $\{ \theta_{\nu}, \theta_{\pi}, \theta_{V} \}$ according to \eqref{opt-1}, \eqref{opt-2}, \eqref{opt-3}, in which $\{ \theta^{n}_{\nu}, \theta^{n}_{\pi}, \theta^{n}_{V} \}$ are replaced by $\{ \theta_{\nu}, \theta_{\pi}, \theta_{V} \}$;
       \STATE $e \leftarrow e+1$;
       \STATE $\text{MO} =  \frac{\sum_{i=1}^{t-1} \text{MO}_{i}}{t-1}$;
       \IF{$\text{MO}> th_{\text{MO}}$}
           \STATE \textbf{break};
       \ENDIF
  \ENDFOR
  \STATE {\bfseries Return:} The trained parameter vectors: $\{ \theta_{\nu}, \theta_{\pi}, \theta_{V} \}$.
\end{algorithmic}
\end{algorithm}

%$\{\mathbf{F}_i\}_{i=1}^{t-1}$
%$\mathbf{F}_t$
%$\mathbf{o}_t$
%$\{\mathbf{o}_i\}_{i=1}^{t-1}$
%$\{(x_i,y_i)\}_{i=1}^{t-1}$
%$\{r_i^{\nu}\}_{i=1}^{t-1}$
%$\{r_i^{\alpha}\}_{i=1}^{t-1}$
%$\{\hat{\alpha}\}_{i=1}^{t-1}$
%$\{\hat{\nu}\}_{i=1}^{t-1}$
%$\hat{\nu}_t$
%$\hat{\alpha}_t$
%$(\hat{x}_{t+1},\hat{y}_{+1})$

\subsection{Stage II: Prediction}
When the average MO is larger than a threshold $th_{\text{mo}}$, the switch in Figure \ref{online-framework} is turned to ``predict'' and the DRL network make a prediction for the action of HM scan-path at frame $t+1$.
Note that if the number of training episodes exceeds $E$, the "predict" is also switched on, so that the training episodes end in a limited time.
When entering the prediction stage, the DRL model trained at the first stage is used to produce the HM position as follows.

First, the LSTM features $\{\mathbf{f}_i\}_{i=1}^{t-1}$ are sequentially updated from frame $1$ to $t-1$, upon the observed FoVs $\{\mathbf{o}\}_{i=1}^{t-1}$ and the DRL parameters $\theta_{\pi}$ of the training stage. Note that the LSTM feature is initialized with the zero vector $\mathbf{0}$ at frame $1$. Then, $\{\mathbf{o}_t, \mathbf{f}_{t-1}, \theta_{\pi}\}$ produces the action $\hat{\alpha}_t$ of HM scan-path direction. In addition, the HM scan-path magnitude $\hat{\nu}_t$ is generated using $\{\mathbf{o}_t, \mathbf{f}_{t-1}, \theta_{\nu}\}$, in which the parameters of $\theta_{\nu}$ are obtained at the training stage. Afterwards, the HM position $(\hat{x}_{t+1}, \hat{y}_{t+1})$ at frame $t+1$ can be predicted, given the ground-truth HM position $(\!x_{t}, \!y_{t}\!)$ and the estimated HM scan-path $(\hat{\alpha}_t, \hat{\nu}_t)$ at frame $t$. Algorithm \ref{online-DHP-algorithm-predicting} presents the summary of the prediction stage in online-DHP. Finally, online-DHP is achieved by alternating between the stages of training and prediction till the last frame.

%HM position $(\!\hat{x}_{t+1}, \!\hat{y}_{t+1}\!)$ at frame $t+1$, upon $\hat{\alpha}_{t},\hat{\nu}_{t}$ and $(\!x_{t}, \!y_{t}\!)$
%
%With the trained DRL model at the first stage, XXX
%
%With the trained DRL model at the first stage, the prediction procedure of online-DHP is,
%\begin{itemize}
%  \item Extract $\mathbf{o}_i$, according to $(x^m_i,y^m_i)$, where $i\in[1,t]$
%  \item Obtain $\mathbf{f}_i$, with above $\mathbf{o}_i$, where $i\in[1,t-1]$
%  \item Feed $\mathbf{f}_{t-1}$ and $\mathbf{o}_{t}$ to DRL model and obtain $\hat{\alpha}_t,\hat{\nu}_t$
%  \item Update $(x^m_t,y^m_t)$ to $(\hat{x}_t,\hat{y}_t)$ according to $\hat{\alpha}_t,\hat{\nu}_t$, which is the online prediction for $t+1$
%\end{itemize}
%
%
%The direction and magnitude of HM scan-path, i.e., $\hat{\alpha}_t$ and $\hat{\nu}_t$, can be obtained at frame $t+1$, as the \textit{action} of DRL. As a result, the HM position $(\hat{x}_{t+1},\hat{y}_{t+1})$ at frame $t+1$ can be predicted, given the ground-truth HM position $(x_{t},y_{t})$ at frame $t$. Algorithm \ref{online-DHP-algorithm-predicting} presents the summary of the prediction stage in online-DHP. Finally, online-DHP is achieved by training and predicting till the last frame.


\begin{algorithm}
   \caption{The algorithm of the prediction stage in online-DHP for frame $t+1$.}
   \label{online-DHP-algorithm-predicting}
   \footnotesize
\begin{algorithmic}
   \STATE {\bfseries Input:} The trained parameter vectors: $\{ \theta_{\nu}, \theta_{\pi}, \theta_{V} \}$ from the training stage, panoramic video frames $\{\mathbf{F}_1, \ldots, \mathbf{F}_t \}$, and the ground-truth HM positions of the \textit{viewer} $\{(x_{1},y_{1}),\ldots, (x_{t},y_{t})\}$;
   %\STATE Load trained parameter vectors from stage training: $\{ \theta_{\hat{\nu}}, \theta_{\hat{\pi}}, \theta_{V} \}$;
   \STATE Initialize the LSTM feature with the zero vector: $\mathbf{f}_0=\mathbf{0}$;
   \FOR{$i=1$ {\bfseries to} $t-1$}
       \STATE Extract \textit{observation} $\mathbf{o}_i$ (FoV) from $\mathbf{F}_i$ according to $(x_{i},y_{i})$;
       \STATE Obtain LSTM feature $\mathbf{f}_{i}$ using the DRL network with $\{\mathbf{o}_{i},\!\mathbf{f}_{i-1},\! \theta_{\pi}\}$;
       \STATE $i \leftarrow i+1$;
   \ENDFOR
   \STATE Extract \textit{observation} $\mathbf{o}_t$ (FoV) from $\mathbf{F}_t$ according to $(x_{t},y_{t})$;
   \STATE Obtain \textit{policy} $\pi_{t}$ using the DRL network with $\{\mathbf{o}_{t},\!\mathbf{f}_{t-1},\! \theta_{\pi}\}$;
   \STATE Choose \textit{action} $\hat{\alpha}_{t}$ with the greedy policy based on $\pi_{t}$;
   \STATE Generate HM magnitude $\hat{\nu}_{t}$ using the DRL network with $\{\mathbf{o}_{t}, \mathbf{f}_{t-1}, \theta_{\nu}\}$;
   \STATE Estimate HM position $(\!\hat{x}_{t+1}, \!\hat{y}_{t+1}\!)$ at frame $t+1$, upon $\hat{\alpha}_{t},\hat{\nu}_{t}$ and $(\!x_{t}, \!y_{t}\!)$;
   \STATE {\bfseries Return:} The HM position at frame $t+1$: $(\hat{x}_{t+1},\hat{y}_{t+1})$.
   \end{algorithmic}
\end{algorithm}


\section{Experimental results}
This section presents experimental results to validate the effectiveness of our offline-DHP and online-DHP approaches. In Section \ref{sec:settings}, we discuss the settings of offline-DHP and online-DHP in our experiments. Sections \ref{sec:evaluation_offline} and \ref{sec:evaluation_online} compares the performance of our offline-DHP and online-DHP approaches with other approaches, in predicting HM positions of offline and online scenarios, respectively.

\subsection{Settings}\label{sec:settings}
For evaluating the performance of offline-DHP, we randomly divided all 76 panoramic video sequences of our PVS-HM database into a training set (61 sequences) and a test set (15 sequences). In training the DRL model, the hyperparameters $\rho$ and $\varrho$ of \eqref{reward-alpha} and \eqref{reward-nu} were tuned over the test set,  to estimate \textit{reward} of HM scan-path prediction. As a result, $\rho$ and $\varrho$ were set to be $42$ and $0.7$. In addition, we followed \cite{mnih2016asynchronous} to set other hyperparameters of DRL. For example, we chose the discount factor $\gamma$ of \eqref{opt-2} and \eqref{opt-3} to be $0.99$ for \textit{reward} optimization. In our experiments, all 61 training sequences, each of which corresponds to a local DRL network, were used to update the global network as the trained DRL model.
The number of DRL workflows $N$ in the offline-DHP framework was set to be 56, the same as the number of subjects in our PVS-HM database.
Similar to \cite{matin1974saccadic},  at each panoramic frame the HM positions predicted by 56 DRL workflows were convoluted with a 2D Gaussian filter, in order to generate the HM map.
In our experiments, the HM maps in a panorama were projected to the 2D coordination for facilitating visualization.

For evaluating the performance of online-DHP, XXX





\subsection{Performance evaluation on offline-DHP}\label{sec:evaluation_offline}
\label{compare}

\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/experiment/objective_result_1}}%of
		\caption{\footnotesize{HM maps of several frames selected from two test sequences in our PVS-HM database. They are all visualized in the 2D coordination.  The second row shows the ground-truth HM maps, which are generated upon HM portions of all 56 subjects. The third to sixth rows show the HM maps of our, BMS \cite{zhang2016exploiting} , OBDL \cite{hossein2015many},and {SALICON} approaches. The last row demonstrates the HM maps by the FCB baseline.}}
		\label{figure-object}
	\end{center}
\end{figure*}
% the CC table
\begin{table*}
    \begin{center}
        \caption{CC results of HM map prediction by our and other approaches}
        \label{table-result}
        \tiny
        \resizebox{\textwidth}{!}{
            \begin{tabular}{cc*{16}{c}c}

				\tabincell{c}{\rotatebox{45}{CC}} & \rotatebox{45}{Method}
					& \rotatebox{45}{StarryPolar} & \rotatebox{45}{Symphony} & \rotatebox{45}{SpaceWar} & \rotatebox{45}{RioOlympics} & \rotatebox{45}{InsideCar}
					& \rotatebox{45}{SpaceWar2} & \rotatebox{45}{Sunset} & \rotatebox{45}{BlueWorld} & \rotatebox{45}{Waterfall} & \rotatebox{45}{Dancing}
					& \rotatebox{45}{CMLauncher2} & \rotatebox{45}{Guitar} & \rotatebox{45}{KingKong} & \rotatebox{45}{BTSRun} & \rotatebox{45}{WaitingForLove}
					& \rotatebox{45}{\textbf{Average}}
                \\
                \toprule

                \multirow{4}{*}{\rotatebox{45}{Non-FCB}}
                \abovespace
    			& Our
    				& 0.185 & 0.710 & 0.573 & 0.717 & \textbf{0.783} & 0.673 & 0.673 & 0.678 & \textbf{0.763} & \textbf{0.837} & 0.585 & 0.645 & 0.751 & 0.764 & 0.471 & \textbf{0.654}
    			\\
    			& BMS
    				& 0.450 & 0.167 & 0.274 & 0.228 & 0.331 & 0.067 & 0.463 & 0.169 & 0.393 & 0.121 & 0.203 & 0.328 & 0.105 & 0.105 & 0.223 & 0.242
    			\\
    			& OBDL
    				& 0.107 & 0.184 & 0.028 & 0.190 & 0.260 & 0.100 & 0.308 & 0.027 & 0.025 & 0.176 & 0.117 & 0.066 & 0.125 & 0.047 & 0.222 & 0.132
    			\\
    			\belowspace
    			& SALICON
    				& 0.293 & 0.129 & 0.126 & 0.153 & 0.364 & 0.034 & 0.186 & 0.265 & 0.103 & 0.120 & 0.166 & 0.216 & 0.150 & 0.063 & 0.256 & 0.175
                \\
                \midrule
                \multirow{4}{*}{\rotatebox{45}{FCB}}
                \abovespace
    			& Our
    				& 0.497 & \textbf{0.816} & 0.574 & 0.768 & 0.712 & 0.655 & \textbf{0.810} & 0.748 & \textbf{0.797} & 0.764 & 0.747 & 0.652 & 0.673 & 0.679 & 0.677 & \textbf{0.704}
    			\\
    			& BMS
    				& 0.692 & 0.567 & 0.520 & 0.494 & 0.495 & 0.368 & \textbf{0.711} & 0.500 & 0.655 & 0.414 & 0.546 & 0.494 & 0.311 & 0.322 & 0.503 & 0.506
    			\\
    			& OBDL
				    & 0.510 & 0.540 & 0.321 & 0.441 & 0.496 & 0.455 & 0.638 & 0.464 & 0.434 & 0.408 & 0.468 & 0.461 & 0.410 & 0.288 & 0.598 & 0.462
			    \\
                \belowspace
    			& SALICON
    				& 0.664 & 0.563 & 0.456 & 0.539 & 0.528 & 0.452 & 0.658 & 0.596 & 0.525 & 0.355 & 0.667 & 0.461 & 0.362 & 0.346 & 0.628 & 0.520
    			\\
                \midrule
    			\multicolumn{2}{c}{FCB Only}
    				\abovespace\belowspace
    				& 0.557 & 0.747 & 0.317 & 0.403 & 0.292 & 0.239 & 0.585 & 0.477 & 0.583 & 0.387 & 0.735 & 0.356 & 0.271 & 0.201 & 0.497 & 0.443
    			\\
                \bottomrule

            \end{tabular}
        }
    \end{center}
\end{table*}

%the Nss table
\begin{table*}
    \begin{center}
        \caption{NSS results of HM map prediction by our and other approaches}
        \label{table-result}
        \tiny
        \resizebox{\textwidth}{!}{
            \begin{tabular}{cc*{16}{c}c}

				\tabincell{c}{\rotatebox{45}{NSS}} & \rotatebox{45}{Method}
					& \rotatebox{45}{StarryPolar} & \rotatebox{45}{RioOlympics} & \rotatebox{45}{SpaceWar2} & \rotatebox{45}{Symphony} & \rotatebox{45}{SpaceWar}
					& \rotatebox{45}{Waterfall} & \rotatebox{45}{Sunset} & \rotatebox{45}{BlueWorld} & \rotatebox{45}{Guitar} & \rotatebox{45}{Dancing}
					& \rotatebox{45}{InsideCar} & \rotatebox{45}{CMLauncher2} & \rotatebox{45}{WaitingForLove} & \rotatebox{45}{BTSRun} & \rotatebox{45}{KingKong}
					& \rotatebox{45}{\textbf{Average}}
                \\
                \toprule

                \multirow{4}{*}{\rotatebox{45}{Non-FCB}}
                \abovespace
     			& Our
    				& 0.899 & 2.806 & 2.237 & 3.346 & 2.180 & 3.765 & 2.529 & 3.196 & \textbf{3.461} & \textbf{5.297} & \textbf{4.402} & 3.529 & 2.278 & 4.572 & 3.334 & \textbf{3.189}
    			\\
    			& BMS
    				& 1.313 & 0.772 & 0.137 & 0.710 & 0.807 & 1.673 & 1.613 & 0.841 & 1.497 & 0.670 & 1.657 & 1.034 & 0.997 & 0.546 & 0.119 & 0.959
    			\\
    			& OBDL
    				& 0.126 & 0.637 & 0.301 & 0.260 & 0.064 & 0.073 & 1.015 & 0.035 & 0.393 & 0.980 & 1.375 & 0.660 & 0.964 & 0.215 & 0.107 & 0.480
    			\\
    			\belowspace
    			& SALICON
    				& 0.730 & 0.600 & 0.251 & 0.456 & 0.344 & 0.410 & 0.669 & 1.138 & 0.965 & 0.230 & 1.823 & 0.921 & 1.298 & 0.337 & 0.203 & 0.692
    			\\
                \midrule
                \multirow{4}{*}{\rotatebox{45}{FCB}}
                \abovespace
    			& Our
    				& 1.825 & 2.911 & 2.064 & \textbf{3.756} & 2.031 & 3.755 & 2.943 & 3.393 & 3.395 & \textbf{4.608} & 3.816 & \textbf{4.463} & 3.351 & 3.931 & 2.883 & \textbf{3.275}
    			\\
    			& BMS
    				& 2.206 & 1.779 & 1.063 & 2.537 & 1.667 & 2.891 & 2.507 & 2.280 & 2.386 & 2.366 & 2.508 & \textbf{3.136} & 2.434 & 1.771 & 1.288 & 2.188
    			\\
    			& OBDL
    				& 1.712 & 1.572 & 1.371 & 2.368 & 1.055 & 1.920 & 2.225 & 2.007 & 2.377 & 2.319 & 2.556 & 2.777 & 2.912 & 1.580 & 1.693 & 2.030
    			\\
    			\belowspace
    			& SALICON
    				& 2.083 & 2.024 & 1.332 & 2.477 & 1.493 & 2.353 & 2.352 & 2.619 & 2.264 & 1.957 & 2.672 & 3.932 & 3.143 & 1.915 & 1.496 & 2.274
    			\\
                \midrule
    			\multicolumn{2}{c}{FCB Only}
    				\abovespace\belowspace
				        & 2.388 & 1.613 & 0.699 & 4.123 & 1.190 & 3.191 & 2.406 & 2.286 & 1.828 & 2.151 & 1.387 & 5.764 & 2.600 & 1.095 & 1.020 & 2.249
    			\\
                \bottomrule

            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/experiment/scanpath_objective_result}}%of
		\caption{\footnotesize{Visualization in scan-paths and positions of HM generated by ground-truth and our offline-DHP approach, for sequence \textit{Help}. The first column shows scan-path of one subject and one DRL workflow. The second and third columns show scan-paths of all 40 subjects and 40 DRL workflows, in longitude and latitude directions. The last column visualizes HM positions of ground-truth and our approach.}}
		\label{scan-path-example}
	\end{center}
\end{figure*}

Now, we evaluate the performance of our offline-DHP approach in predicting HM maps of all 15 test sequences from the PVS-HM database. To the best of our knowledge, there is no work on predicting HM maps of panoramic video, and saliency prediction is the closest field. Therefore, we compare our offline-DHP approach with three state-of-the-art saliency detection approaches, i.e.,  OBDL  \cite{hossein2015many}, BMS \cite{zhang2016exploiting} and SALICON\cite{huang2015salicon}.
In particular, OBDL  \cite{hossein2015many} and BMS \cite{zhang2016exploiting} are the latest saliency detection approaches for video and image, respectively.
SALICON\cite{huang2015salicon} is a state-of-the-art DNN approach for saliency detection.
In addition to the above three approaches, we also compare our approach to the front-center-bias (FCB) baseline, since human attention normally biases towards front-center regions of panoramic video.
Here, we model FCB by Gaussian distribution\footnote{The standard deviation is 12.8 degree, obtained by least square fitting over all HM positions of the training set.}, similar to the center bias of saliency detection.
In the field of saliency detection, the center bias  \cite{borji2013state} is normally multiplied to saliency maps for improving saliency detection accuracy. Hence, we further report results of HM maps \textcolor{blue}{combined}/multiplied by FCB, for our and other approaches.
For evaluation, we measure the accuracy of HM map prediction in terms of CC and normalized scan-path saliency (NSS), which are two effective evaluation metrics \textcolor{blue}{\cite{Li_2015_ICCV}} in saliency detection.
Note that the larger value of CC or NSS means more accurate prediction of HM maps.

Table \ref{table-result} tabulates the results of CC and NSS in predicting HM maps of 15 test sequences, for our and other approaches.
In this table, the results of CC and NSS are averaged over all frames for each test sequence.
We can see from this table that our offline-DHP approach is drastically superior to other approaches, when FCB is not integrated.
Specifically, our  offline-DHP approach increases the CC value averaged over all 15 test sequences by 0.522, 0.412 and 0.479, compared with OBDL, BMS and SALICON, respectively.
Additionally, the increase of averaged NSS value is 2.709, 2.230 and 2.497 in offline-DHP, over OBDL, BMS and SALICON.
Once integrated with FCB, all four approaches have performance improvement, and our approach still performs best among all three approaches. Meanwhile, our approach significantly outperforms the FCB baseline.
In a word, our offline-DHP approach is effective in predicting HM maps of panoramic video, much better than other approaches and the FCB baseline.


Next, we move to the comparison of subjective results. Figure \ref{figure-object} shows several frames from two selected sequences and their ground-truth HM maps of human.
In Figure \ref{figure-object}, we further visualize the HM maps generated by our and other approaches including the FCB baseline. Here, the predicted HM maps are \textcolor{blue}{integrated with FCB}, as FCB can improve the performance of all four approaches in predicting HM maps (as seen in Table \ref{table-result}).
From this figure, one can observe that HM maps of our approach are more close to the ground-truth HM maps, compared to other approaches and the FCB baseline.
This indicates that our offline-DHP approach is capable of better locating HM positions of different subjects on panoramic video.
Moreover, Figure \ref{scan-path-example} plots the HM scan-paths by different subjects and by multiple DRL workflows of offline-DHP, to investigate the agreement between the subjects and DRL workflows.
We can see from this figure that the DRL workflows are able to yield similar scan-paths as humans.  We further show in Figure \ref{scan-path-example} the HM positions obtained from the ground-truth and predicted HM scan-paths. This figure reveals that HM positions can be well predicted by our offline-DHP approach. In conclusion, our offline-DHP approach is effective in modeling HM maps by predicting both scan-paths and positions of HM.


%Iceclear online

\subsection{Performance evaluation on online-DHP}\label{sec:evaluation_online}
\label{online-compare}

%\begin{table*}
%	\begin{center}
%		\caption{MO results of HM position prediction by our and other approaches} \centerline{\includegraphics[width=2\columnwidth]{figures/experiment_on_line/Online_compare}}%of
%		\label{table-MeanMo}
%	\end{center}
%\end{table*}

% the Ice Table
\begin{table*}
    \begin{center}
        \caption{MO results of HM position prediction by our and other approaches}
        \label{table-result}
        \tiny
        \resizebox{\textwidth}{!}{
            \begin{tabular}{cc*{16}{c}c}

				\tabincell{c}{\rotatebox{45}{Method}}
					& \rotatebox{45}{KingKong} & \rotatebox{45}{SpaceWar2} & \rotatebox{45}{StarryPolar} & \rotatebox{45}{Dancing} & \rotatebox{45}{Guitar}
					& \rotatebox{45}{BTSRun} & \rotatebox{45}{InsideCar} & \rotatebox{45}{RioOlympics} & \rotatebox{45}{SpaceWar} & \rotatebox{45}{CMLauncher2}
					& \rotatebox{45}{Waterfall} & \rotatebox{45}{Sunset} & \rotatebox{45}{BlueWorld} & \rotatebox{45}{Symphony} & \rotatebox{45}{WaitingForLove} & \rotatebox{45}{{\textbf{Average}}}

                \\
               \toprule
               \multirow{1}{*}{\rotatebox{0}{Online}}
               \abovespace
     			
    			  & 0.809 & 0.763 & 0.549 & 0.859 & 0.785 & 0.878 & 0.847 & 0.820 & 0.626 & 0.763 & 0.667 & 0.659 & 0.693 & 0.747 & 0.836 & 0.753
    			\\
                \multirow{1}{*}{\rotatebox{0}{Deep 360 Polit}}
    			  & 0.266 & 0.285 & 0.258 & 0.386 & 0.341 & 0.381 & 0.258 & 0.240 & 0.298 & 0.270 & 0.274 & 0.283 & 0.314 & 0.432 & 0.393 & 0.312
    			\\
                 \multirow{1}{*}{\rotatebox{0}{Random-baseline}}
    			 & 0.201 & 0.206 & 0.161 & 0.216 & 0.203 & 0.206 & 0.216 & 0.203 & 0.209 & 0.205 & 0.203 & 0.204 & 0.206 & 0.202 & 0.211 & 0.204
    			\\
    			\belowspace
                 \multirow{1}{*}{\rotatebox{0}{Keep-baseline}}
    			 & 0.224 & 0.231 & 0.197 & 0.217 & 0.227 & 0.237 & 0.234 & 0.225 & 0.216 & 0.251 & 0.251 & 0.209 & 0.229 & 0.216 & 0.225 & 0.226
    			\\
                \bottomrule
            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{figure*}
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{figures/experiment_on_line/Mean_MO}}%of
		\caption{\footnotesize{MO results between Online-DHP and Online-DHP without offline-DHP.}}
		\label{Online_compare}
	\end{center}
\end{figure*}

We evaluate the performance of our online-DHP approach for predicting HM positions in the online scenario.
The online scenario refers to predicting HM position of one subject at each frame upon the observed HM positions of this subject at the previous frames.
In our experiments, we compare the performance of online-DHP with the state-of-the-art deep 360 pilot \cite{hu2017deep}, which is the only existing approach for the online prediction of HM positions.
We also compare our online-DHP with two baselines. According to \textit{Finding 5}, the first baseline (called baseline 1) keeps the HM scan-path of the current time interval the same as that at the previous time interval, such that the HM position at each frame can be generated.
The second baseline (called baseline 2) produces the HM position, using the randomly generated HM scan-path. The same as \cite{hu2017deep}, MO of \eqref{mo-defination} is measured as the metric to evaluate the performance of online prediction for HM positions.
%The performance evaluation is conducted on the test set of our PVS-HM database.
%More specifically, the HM positions of 58 subjects on the 15 test sequences of the PVS-HM are all tested.

Table \ref{table-MeanMo} compares the MO results of our and other approaches for the 15 test sequences of our PVS-HM database. Note that the MO results of each sequence are averaged over the predicted HM positions of all 58 subjects in our database. As observed in this table, our online-DHP approach is significantly superior to two baselines, indicating the effectiveness of applying DRL in predicting HM positions online.
In addition, we can see from Table \ref{table-MeanMo} that our online-DHP approach performs better than the deep 360 pilot \cite{hu2017deep}, with an increase of XXXX in average MO. Such a increase is probably because (1) the online DRL model of our approach is capable of generating the accurate \textit{actions} of HM scan-paths. and (2) the DRL network of offline-DHP is incorporated in our online prediction as the prior knowledge.

To test the generalizability of our approach, we further evaluate the performance of our, the deep 360 pilot \cite{hu2017deep} and two baseline approaches on the test set of \cite{hu2017deep}. For such an evaluation, our online-DHP is still based on our offline DRL network that is learned from the training sequences of our PVS-HM database. The MO results are shown in Table XXX. From this table, we can see that our online-DHP approach again outperforms \cite{hu2017deep} and two baselines. In particular, our approach has XXX MO increase over \cite{hu2017deep}, despite testing on the test set of \cite{hu2017deep}. Therefore, the generalization capability of our approach can be confirmed.

It is interesting to analyze the gain of incorporating the DRL network of offline-DHP in our online-DHP approach, since the online-DHP approach is based on the offline DRL network. Figure \ref{Online_compare} shows the MO results of our online-DHP approach with and without the offline DRL network. As observed in this figure, the offline DRL network is able to increase MO results of our online-DHP approach, for all 15 sequences. In addition, the MO value can be increased from 0.502 to 0.753 on average, when the offline DRL network is incorporated in online-DHP. Therefore, the learned DRL network of offline-DHL also benefits the online prediction of HM positions in online-DHL.

In online-DHP, the performance of online HM prediction is related to the hyperparameters of $th_{\text{MO}}$, XXX, and XXX. In the following, we analyze the influences of this hyperparameters on the performance of online-DHP. Figure XXX shows the MO results at various values of $th_{\text{MO}}$, XXX, and XXX. The hyperparameter $th_{\text{MO}}$ is the threshold of the MO value achieved by XXX, and it determines whether the \textit{action} of online-DHP is output as the HM scan-path at the current frame. We can see from Figure XXX that XXX is increased at $th_{\text{MO}}\in XXX$ and converged once $th_{\text{MO}}\geq???$. Thus, in this paper, we set $th_{\text{MO}}$ to be XXX.


\textcolor{blue}{Now, we move to the evaluation of Online-DHP. We compare our Online-DHP approach with Deep 360 Pilot approach\cite{hu2017deep} and two baselines.}

\textcolor{blue}{Table \ref{table-MeanMo} compares mean MO results of our and other approaches, in predicting HM positions over all 15 test sequences of panoramic video.
Here, MO results are averaged over all frames for each test sequence.
The Random-baseline means predicting the HM position in a totally random way.
And the Keep-baseline means predicting the HM position due to the HM position of the former frame.
We can see from this table that our approach performs far better than other approaches. Specifically, our approach has ? average increment over Deep 360 Pilot approach\cite{hu2017deep}.
We think that is mostly because that our approach has priori knowledge from the Offline-DHP.
For the two baselines, the Keep-baseline is a little better than the Random-baseline mostly because of the guidance of the former frame.
Compared with the two baselines, our approach is also obviously much more effective.}

\textcolor{blue}{Offline-DHP is an important component of our approach.
Figure \ref{Online_compare} shows the result of mean MO between Online-DHP and Online-DHP without Offline-DHP.
We can see that the Offline-DHP enhances the performance of online prediction a lot.
We think it's mostly because the Offline-DHP provides priori knowledge for the online prediction.
And this is very effective in predicting.}

\textcolor{blue}{After the comparison, we move to the analysis of an important parameter $th_{MO}$ in algorithm \ref{online-DHP-algorithm}, which affects both the result of MO\eqref{mo-defination} and the time complexity of our approach.
We call it the threshold of MO.
According to our approach, we think the threshold $th_{MO}$ is positively correlated with the result of MO, but negatively correlated with the time complexity.}



\section{Conclusion}
In this paper, we have proposed the DHP approach to predict HM positions on panoramic video. First, we established a new database named PVS-HM, which includes HM data of 58 subjects in 76 panoramic sequences. We found for our database that the HM positions are highly consistent across humans. Thus, the consistent HM positions on each panoramic frame can be represented in the form of the HM map, which encodes possibility of each pixel being HM position. Second, we proposed the offline-DHP approach to estimate HM maps in an offline manner. To be more specific, our offline-DHP approach leverages DRL to make decision on \textit {actions} of HM scan-paths. Afterwards, HM scan-paths of several \textit{agents} from multiple DRL workflows are integrated to yield final HM maps.
Third, we proposed the online-DHP approach, which online predicts the HM positions of one subject. In online-DHP, the DRL algorithm was developed in determining the HM positions of one \textit{agent} at the incoming frames, given \textit{observation} of the previous HM scan-path and current video content. The DRL algorithm is based on the learned model of offline-DHP in extracting spatio-temporal features of attention-related content. Finally, the experimental results showed that offline-DHP and online-DHP are superior to other conventional approaches, in the offline and online tasks of HM prediction for panoramic video.

Humans always perceive the world around them in a panorama, rather than the 2D plane. Therefore, modelling attention on panoramic video is an important component in establishing human-like computer vision systems. Our work at the current stage mainly focuses on predicting HM positions, as the first step towards attention modelling of panoramic video. The future work should further predict eye fixations within FoV regions of panoramic video. The potential applications of our approach in human-like computer vision systems are another promising work in future. For example, the online-DHP approach may be embedded in robotics, to mimic human's way in perceiving the real world. Besides, panoramic video has large perceptual redundancy, since most of panoramic regions cannot be seen by humans. It is thus possible to use the offline-DHP approach to remove such perceptual redundancy, for saving bit-rates of panoramic video coding.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of Proposition 1}

Example.




\subsection{The Analysis of FCB Combined in HM Maps}
The literature of saliency detection \cite{judd2009learning} has argued that human attention has strong center bias in images or videos, and that the incorporation of center bias can improve the performance of saliency detection. Similarly, there exists FCB in viewing panoramic videos, as discussed in \textit{Finding 1}. Hence, this appendix discusses the combination of the offline-DHP approach and the FCB feature. Here, we apply the FCB feature as an additional channel in generating HM maps of panoramic videos. Specifically, assume that $\mathbf{H}^f$ is the HM map of the FCB feature. Similar to the center bias feature of image saliency detection \cite{judd2009learning}, we apply the following 2D Gaussian distribution to model $\mathbf{H}^f$ for each frame,
\begin{equation}
\label{Gauss_sigma}
\mathbf{H}^f(u,v)= \exp\left({-\frac{(u-u_f)^2+(v-v_f)^2}{\sigma_f^2}}\right),
\end{equation}
where $(u,v)$ is the longitude and latitude of the GDS location in the map, and $(u_f,v_f)$ is the longitude and latitude of the front center position in GDS. In addition, $\sigma_f$ is the standard deviation of  the 2D Gaussian distribution.

Next, we need to combine $\mathbf{H}^f$ with the predicted HM map $\mathbf{H}_t$ for each panoramic frame by
\begin{equation}
\label{optmization_x}
\mathbf{H}^c_t = w_1\cdot \mathbf{H}^f+w_2\cdot \mathbf{H}_t,
\end{equation}
where $\mathbf{H}^c_t$ is the HM map integrated with the FCB feature for frame $t$. In \eqref{optmization_x}, $w_1$ and $w_2$ are the weights corresponding to the channels of $\mathbf{H}_c$ and $\mathbf{H}_t$, respectively. Given \eqref{Gauss_sigma} and \eqref{optmization_x}, the following optimization formulation is applied to obtain the values of $\sigma_f$, $w_1$ and $w_2$:
\begin{equation}
\label{optmization_w}
\min_{w_1,w_2} \sum_{t=1}^{T} \text{CC}(\mathbf{H}^c_t, \mathbf{H}^g_t), \quad \text{s.t.} \quad w_1+w_2=1.
\end{equation}
In \eqref{optmization_x}, $\mathbf{H}^g_t$ is the ground-truth HM map of each frame; $\text{CC}(\cdot,\cdot)$ indicates the CC value of two maps. Then, we solve the above optimization formulation by the least square fitting over all training data of our PVS-HM database. Consequently, the optimal values of $\sigma_f$, $w_1$ and $w_2$ are $21.1^\circ$, $0.52$ and $0.48$, respectively. These values are used in integrating the FCB feature in our offline-FCB approach. Note that the same way is applied to obtain weights of $w_1$ and $w_2$ when combining the FCB feature with other approaches.

Figure \ref{fitting_surface} shows the results of  CC between the predicted and ground-truth HM maps at various values of $\sigma_f$ and $w_1$. From this figure, we can see that CC varies from $0.44$ to $0.70$ alongside the increase of $w_1$ from $0$ to $1$, reaching the maximum value at $\sigma_f=21.1^\circ$ and $w_1=0.52$. This indicates that both the FCB feature and our offline-DHP approach are effective in predicting the HM maps of panoramic video, and that the effectiveness of the FCB feature is different at varying combination weights. In addition, as shown in Figure \ref{fitting_surface}, at $w_1=0.52$, the CC value increases from 0.66 to 0.70, when $\sigma_f$ grows from $7^\circ$ to $21.1^\circ$, and then it decreases to 0.63 till $\sigma_f = 43.6^\circ$. Thus, the standard deviation of the 2D Gaussian distribution is set to be $21.1^\circ$ for the FCB feature in our experiments.


\begin{figure}
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{figures/experiment/Fitting}}%of
		\caption{\footnotesize{The fitting surface of CC between the predicted and ground-truth HM maps at various $\sigma_f$ and $w_1$. The dark dots in this figure represent the CC results at each specific value of $\sigma_f$ and $w_1$, which are used to fit the surface. Note that the CC results are obtained over all training data of the PVS-HM database}}
		\label{fitting_surface}
	\end{center}
\end{figure}





% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
\bibliography{pami2017_dhp}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}
